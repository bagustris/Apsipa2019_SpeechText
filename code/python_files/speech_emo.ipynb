{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICSigSys4: Speech emotion recognition using voiced speech and local attention\n",
    "by Bagus Tris Atmaja (bagus@jaist.ac.jp)  \n",
    "\n",
    "In this paper we compare the following model for speech emotion recognition:\n",
    "1. Raw speech, using 2 stack Bidirectional LSTM\n",
    "2. Voiced speech only, using 2 stack Bidirectional LSTM\n",
    "3. Raw speech, using 2 stack Bidirectional LSTM and attention model\n",
    "4. Voiced speech, using 2 stack Bidirectional LSTM and attention model\n",
    "\n",
    "For silence removal, we evaluate the following variations:\n",
    "- Effect of various minimum sample duration for silence: 0.001, 0.006, 0.01 s\n",
    "- Effect of various minimum threshold for silence to be removed: 0.001, 0.007, 0.01 %\n",
    "\n",
    "that lead 0.001 minimum duration and 0.001 of threshold giving the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import needed module, unimport unneeded moduld\n",
    "import numpy as np\n",
    "from tensorflow import set_random_seed\n",
    "import matplotlib.pyplot as plt\n",
    "import audiosegment\n",
    "import sounddevice as sd\n",
    "from features import *\n",
    "from helper import *\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import signal\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import wave\n",
    "import copy\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import LSTM, Input, Flatten, Embedding, Conv1D, Conv2D, Dropout, MaxPooling2D, Bidirectional\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "set_random_seed(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir path for the data\n",
    "code_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "emotions_used = np.array(['ang', 'exc', 'neu', 'sad'])\n",
    "data_path = \"/media/bagus/data01/dataset/IEMOCAP_full_release/\"\n",
    "sessions = ['Session1', 'Session2', 'Session3', 'Session4', 'Session5']\n",
    "framerate = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read already saved pickle data\n",
    "import pickle\n",
    "with open(data_path +'data_collected.pickle', 'rb') as handle:\n",
    "    data2 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calc feature\n",
    "\n",
    "def calculate_features(frames, freq, options):\n",
    "    window_sec = 0.2\n",
    "    window_n = int(freq * window_sec)\n",
    "\n",
    "    st_f = stFeatureExtraction(frames, freq, window_n, window_n / 2)\n",
    "\n",
    "    if st_f.shape[1] > 2:\n",
    "        i0 = 1\n",
    "        i1 = st_f.shape[1] - 1\n",
    "        if i1 - i0 < 1:\n",
    "            i1 = i0 + 1\n",
    "        \n",
    "        deriv_st_f = np.zeros((st_f.shape[0], i1 - i0), dtype=float)\n",
    "        for i in range(i0, i1):\n",
    "            i_left = i - 1\n",
    "            i_right = i + 1\n",
    "            deriv_st_f[:st_f.shape[0], i - i0] = st_f[:, i]\n",
    "        return deriv_st_f\n",
    "    elif st_f.shape[1] == 2:\n",
    "        deriv_st_f = np.zeros((st_f.shape[0], 1), dtype=float)\n",
    "        deriv_st_f[:st_f.shape[0], 0] = st_f[:, 0]\n",
    "        return deriv_st_f\n",
    "    else:\n",
    "        deriv_st_f = np.zeros((st_f.shape[0], 1), dtype=float)\n",
    "        deriv_st_f[:st_f.shape[0], 0] = st_f[:, 0]\n",
    "        return deriv_st_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read speech signal from variable data2\n",
    "x_train_speech = np.load('voiced_feat_without_sil_removal.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4936, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read output label\n",
    "Y=[]\n",
    "for ses_mod in data2:\n",
    "    Y.append(ses_mod['emotion'])\n",
    "    \n",
    "Y = label_binarize(Y,emotions_used)\n",
    "\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1\n",
    "def speech_model1():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(100, 34)))\n",
    "    model.add(LSTM(256, return_sequences=False))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(4))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100, 512)          1120256   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 2,041,348\n",
      "Trainable params: 2,041,348\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = speech_model1()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3307 samples, validate on 1629 samples\n",
      "Epoch 1/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0582 - acc: 0.9831 - val_loss: 3.8061 - val_acc: 0.5629\n",
      "Epoch 2/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0641 - acc: 0.9825 - val_loss: 3.7052 - val_acc: 0.5574\n",
      "Epoch 3/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0383 - acc: 0.9894 - val_loss: 3.6128 - val_acc: 0.5494\n",
      "Epoch 4/50\n",
      "3307/3307 [==============================] - 29s 9ms/step - loss: 0.0659 - acc: 0.9834 - val_loss: 4.1769 - val_acc: 0.5445\n",
      "Epoch 5/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0834 - acc: 0.9791 - val_loss: 3.5950 - val_acc: 0.5568\n",
      "Epoch 6/50\n",
      "3307/3307 [==============================] - 29s 9ms/step - loss: 0.0585 - acc: 0.9876 - val_loss: 3.6311 - val_acc: 0.5427\n",
      "Epoch 7/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0573 - acc: 0.9834 - val_loss: 3.7009 - val_acc: 0.5537\n",
      "Epoch 8/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0619 - acc: 0.9837 - val_loss: 3.8587 - val_acc: 0.5770\n",
      "Epoch 9/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0710 - acc: 0.9816 - val_loss: 4.0386 - val_acc: 0.5396\n",
      "Epoch 10/50\n",
      "3307/3307 [==============================] - 29s 9ms/step - loss: 0.0628 - acc: 0.9831 - val_loss: 3.8385 - val_acc: 0.5599\n",
      "Epoch 11/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0559 - acc: 0.9861 - val_loss: 4.0759 - val_acc: 0.5629\n",
      "Epoch 12/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0749 - acc: 0.9831 - val_loss: 3.9197 - val_acc: 0.5562\n",
      "Epoch 13/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0515 - acc: 0.9852 - val_loss: 4.1742 - val_acc: 0.5457\n",
      "Epoch 14/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0593 - acc: 0.9846 - val_loss: 3.3269 - val_acc: 0.5580\n",
      "Epoch 15/50\n",
      "3307/3307 [==============================] - 29s 9ms/step - loss: 0.0651 - acc: 0.9858 - val_loss: 3.8091 - val_acc: 0.5488\n",
      "Epoch 16/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0671 - acc: 0.9809 - val_loss: 3.8347 - val_acc: 0.5347\n",
      "Epoch 17/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0570 - acc: 0.9870 - val_loss: 3.7425 - val_acc: 0.5506\n",
      "Epoch 18/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0580 - acc: 0.9849 - val_loss: 3.9881 - val_acc: 0.5592\n",
      "Epoch 19/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0652 - acc: 0.9849 - val_loss: 3.5984 - val_acc: 0.5562\n",
      "Epoch 20/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0406 - acc: 0.9882 - val_loss: 4.0359 - val_acc: 0.5421\n",
      "Epoch 21/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0482 - acc: 0.9882 - val_loss: 4.1852 - val_acc: 0.5408\n",
      "Epoch 22/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0757 - acc: 0.9809 - val_loss: 3.4968 - val_acc: 0.5709\n",
      "Epoch 23/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0603 - acc: 0.9843 - val_loss: 3.9751 - val_acc: 0.5396\n",
      "Epoch 24/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0637 - acc: 0.9849 - val_loss: 4.1668 - val_acc: 0.5353\n",
      "Epoch 25/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0690 - acc: 0.9849 - val_loss: 3.5937 - val_acc: 0.5617\n",
      "Epoch 26/50\n",
      "3307/3307 [==============================] - 29s 9ms/step - loss: 0.0478 - acc: 0.9873 - val_loss: 3.8483 - val_acc: 0.5519\n",
      "Epoch 27/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0421 - acc: 0.9894 - val_loss: 4.0645 - val_acc: 0.5482\n",
      "Epoch 28/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0760 - acc: 0.9846 - val_loss: 3.7117 - val_acc: 0.5463\n",
      "Epoch 29/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0550 - acc: 0.9846 - val_loss: 4.3730 - val_acc: 0.5635\n",
      "Epoch 30/50\n",
      "3307/3307 [==============================] - 29s 9ms/step - loss: 0.0527 - acc: 0.9870 - val_loss: 4.4669 - val_acc: 0.5365\n",
      "Epoch 31/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0891 - acc: 0.9791 - val_loss: 3.8696 - val_acc: 0.5537\n",
      "Epoch 32/50\n",
      "3307/3307 [==============================] - 29s 9ms/step - loss: 0.0459 - acc: 0.9897 - val_loss: 3.9885 - val_acc: 0.5568\n",
      "Epoch 33/50\n",
      "3307/3307 [==============================] - 29s 9ms/step - loss: 0.0585 - acc: 0.9867 - val_loss: 4.3274 - val_acc: 0.5285\n",
      "Epoch 34/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0696 - acc: 0.9846 - val_loss: 4.2201 - val_acc: 0.5144\n",
      "Epoch 35/50\n",
      "3307/3307 [==============================] - 29s 9ms/step - loss: 0.0694 - acc: 0.9803 - val_loss: 3.6733 - val_acc: 0.5513\n",
      "Epoch 36/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0429 - acc: 0.9882 - val_loss: 4.1566 - val_acc: 0.5494\n",
      "Epoch 37/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0694 - acc: 0.9828 - val_loss: 4.0937 - val_acc: 0.5396\n",
      "Epoch 38/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0398 - acc: 0.9885 - val_loss: 4.5939 - val_acc: 0.5328\n",
      "Epoch 39/50\n",
      "3307/3307 [==============================] - 29s 9ms/step - loss: 0.0592 - acc: 0.9840 - val_loss: 3.6444 - val_acc: 0.5421\n",
      "Epoch 40/50\n",
      "3307/3307 [==============================] - 29s 9ms/step - loss: 0.0450 - acc: 0.9906 - val_loss: 4.3630 - val_acc: 0.5599\n",
      "Epoch 41/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0547 - acc: 0.9864 - val_loss: 3.7863 - val_acc: 0.5586\n",
      "Epoch 42/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0391 - acc: 0.9885 - val_loss: 4.1369 - val_acc: 0.5488\n",
      "Epoch 43/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0471 - acc: 0.9882 - val_loss: 3.9056 - val_acc: 0.5599\n",
      "Epoch 44/50\n",
      "3307/3307 [==============================] - 29s 9ms/step - loss: 0.0586 - acc: 0.9819 - val_loss: 3.6408 - val_acc: 0.5525\n",
      "Epoch 45/50\n",
      "3307/3307 [==============================] - 30s 9ms/step - loss: 0.0386 - acc: 0.9888 - val_loss: 4.0393 - val_acc: 0.5359\n",
      "Epoch 46/50\n",
      "3307/3307 [==============================] - 29s 9ms/step - loss: 0.0360 - acc: 0.9903 - val_loss: 4.0220 - val_acc: 0.5513\n",
      "Epoch 47/50\n",
      "3307/3307 [==============================] - 29s 9ms/step - loss: 0.0527 - acc: 0.9852 - val_loss: 3.7964 - val_acc: 0.5457\n",
      "Epoch 48/50\n",
      "3307/3307 [==============================] - 29s 9ms/step - loss: 0.0437 - acc: 0.9906 - val_loss: 3.9234 - val_acc: 0.5660\n",
      "Epoch 49/50\n",
      "3307/3307 [==============================] - 29s 9ms/step - loss: 0.0553 - acc: 0.9873 - val_loss: 3.9440 - val_acc: 0.5439\n",
      "Epoch 50/50\n",
      "3307/3307 [==============================] - 29s 9ms/step - loss: 0.0377 - acc: 0.9903 - val_loss: 4.1207 - val_acc: 0.5543\n"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "hist = model1.fit(x_train_speech, Y, batch_size=32, epochs=50, verbose=1, shuffle=False,\n",
    "                 validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5499693062625244 0.5770411293626636\n"
     ]
    }
   ],
   "source": [
    "acc1 = hist.history['val_acc']\n",
    "print(np.mean(acc1), max(acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# doing silence removal (already saved as npy)\n",
    "# voiced_feat = []\n",
    "# for ses_mod in data2:\n",
    "#     x_head = ses_mod['signal']\n",
    "#     seg = audiosegment.from_numpy_array(x_head, framerate)\n",
    "#     voice = seg.filter_silence(0.01, 0.1)\n",
    "#     st_features = calculate_features(voice.to_numpy_array(), framerate, None)\n",
    "#     st_features, _ = pad_sequence_into_array(st_features, maxlen=100)\n",
    "#     voiced_feat.append( st_features.T )\n",
    "#     #print(i)\n",
    "    \n",
    "# voiced_feat = np.array(voiced_feat)\n",
    "# voiced_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "voiced_feat = np.load('voiced_feat_file_001_001.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3307 samples, validate on 1629 samples\n",
      "Epoch 1/50\n",
      "3307/3307 [==============================] - 53s 16ms/step - loss: 0.0710 - acc: 0.9782 - val_loss: 3.1209 - val_acc: 0.5697\n",
      "Epoch 2/50\n",
      "3307/3307 [==============================] - 57s 17ms/step - loss: 0.0919 - acc: 0.9743 - val_loss: 3.3989 - val_acc: 0.5697\n",
      "Epoch 3/50\n",
      "3307/3307 [==============================] - 53s 16ms/step - loss: 0.0860 - acc: 0.9737 - val_loss: 3.2052 - val_acc: 0.5820\n",
      "Epoch 4/50\n",
      "3307/3307 [==============================] - 57s 17ms/step - loss: 0.0969 - acc: 0.9755 - val_loss: 3.3356 - val_acc: 0.5353\n",
      "Epoch 5/50\n",
      "3307/3307 [==============================] - 54s 16ms/step - loss: 0.0892 - acc: 0.9713 - val_loss: 3.0299 - val_acc: 0.5666\n",
      "Epoch 6/50\n",
      "3307/3307 [==============================] - 57s 17ms/step - loss: 0.0810 - acc: 0.9767 - val_loss: 2.9335 - val_acc: 0.5684\n",
      "Epoch 7/50\n",
      "3307/3307 [==============================] - 53s 16ms/step - loss: 0.0529 - acc: 0.9813 - val_loss: 3.4897 - val_acc: 0.5623\n",
      "Epoch 8/50\n",
      "3307/3307 [==============================] - 57s 17ms/step - loss: 0.0959 - acc: 0.9728 - val_loss: 3.1771 - val_acc: 0.5740\n",
      "Epoch 9/50\n",
      "3307/3307 [==============================] - 52s 16ms/step - loss: 0.0832 - acc: 0.9770 - val_loss: 2.8597 - val_acc: 0.5801\n",
      "Epoch 10/50\n",
      "3307/3307 [==============================] - 57s 17ms/step - loss: 0.0807 - acc: 0.9773 - val_loss: 3.0991 - val_acc: 0.5727\n",
      "Epoch 11/50\n",
      "3307/3307 [==============================] - 54s 16ms/step - loss: 0.0707 - acc: 0.9806 - val_loss: 3.3614 - val_acc: 0.5734\n",
      "Epoch 12/50\n",
      "3307/3307 [==============================] - 40s 12ms/step - loss: 0.0803 - acc: 0.9761 - val_loss: 3.0507 - val_acc: 0.5734\n",
      "Epoch 13/50\n",
      "3307/3307 [==============================] - 36s 11ms/step - loss: 0.0865 - acc: 0.9746 - val_loss: 3.1249 - val_acc: 0.5783\n",
      "Epoch 14/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.1030 - acc: 0.9755 - val_loss: 3.4865 - val_acc: 0.5574\n",
      "Epoch 15/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0678 - acc: 0.9809 - val_loss: 3.0801 - val_acc: 0.5709\n",
      "Epoch 16/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0942 - acc: 0.9785 - val_loss: 2.9601 - val_acc: 0.5838\n",
      "Epoch 17/50\n",
      "3307/3307 [==============================] - 36s 11ms/step - loss: 0.0803 - acc: 0.9776 - val_loss: 3.3277 - val_acc: 0.5549\n",
      "Epoch 18/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0773 - acc: 0.9803 - val_loss: 3.2223 - val_acc: 0.5592\n",
      "Epoch 19/50\n",
      "3307/3307 [==============================] - 50s 15ms/step - loss: 0.0783 - acc: 0.9755 - val_loss: 3.4089 - val_acc: 0.5549\n",
      "Epoch 20/50\n",
      "3307/3307 [==============================] - 56s 17ms/step - loss: 0.0641 - acc: 0.9816 - val_loss: 3.7311 - val_acc: 0.5574\n",
      "Epoch 21/50\n",
      "3307/3307 [==============================] - 53s 16ms/step - loss: 0.0797 - acc: 0.9785 - val_loss: 3.1964 - val_acc: 0.5697\n",
      "Epoch 22/50\n",
      "3307/3307 [==============================] - 54s 16ms/step - loss: 0.0691 - acc: 0.9837 - val_loss: 3.3646 - val_acc: 0.5721\n",
      "Epoch 23/50\n",
      "3307/3307 [==============================] - 57s 17ms/step - loss: 0.0726 - acc: 0.9822 - val_loss: 3.4403 - val_acc: 0.5887\n",
      "Epoch 24/50\n",
      "3307/3307 [==============================] - 53s 16ms/step - loss: 0.1131 - acc: 0.9725 - val_loss: 2.8775 - val_acc: 0.5758\n",
      "Epoch 25/50\n",
      "3307/3307 [==============================] - 57s 17ms/step - loss: 0.0667 - acc: 0.9816 - val_loss: 3.1193 - val_acc: 0.5586\n",
      "Epoch 26/50\n",
      "3307/3307 [==============================] - 53s 16ms/step - loss: 0.0653 - acc: 0.9816 - val_loss: 3.4068 - val_acc: 0.5513\n",
      "Epoch 27/50\n",
      "3307/3307 [==============================] - 56s 17ms/step - loss: 0.0654 - acc: 0.9785 - val_loss: 3.3486 - val_acc: 0.5752\n",
      "Epoch 28/50\n",
      "3307/3307 [==============================] - 53s 16ms/step - loss: 0.0749 - acc: 0.9791 - val_loss: 3.2045 - val_acc: 0.5770\n",
      "Epoch 29/50\n",
      "3307/3307 [==============================] - 40s 12ms/step - loss: 0.0767 - acc: 0.9791 - val_loss: 3.4091 - val_acc: 0.5734\n",
      "Epoch 30/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0577 - acc: 0.9816 - val_loss: 3.3127 - val_acc: 0.5543\n",
      "Epoch 31/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0525 - acc: 0.9840 - val_loss: 3.6345 - val_acc: 0.5635\n",
      "Epoch 32/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0752 - acc: 0.9782 - val_loss: 3.3301 - val_acc: 0.5592\n",
      "Epoch 33/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0643 - acc: 0.9831 - val_loss: 3.7100 - val_acc: 0.5488\n",
      "Epoch 34/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0608 - acc: 0.9846 - val_loss: 3.6138 - val_acc: 0.5568\n",
      "Epoch 35/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0651 - acc: 0.9791 - val_loss: 3.5144 - val_acc: 0.5611\n",
      "Epoch 36/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0627 - acc: 0.9840 - val_loss: 3.8386 - val_acc: 0.5605\n",
      "Epoch 37/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0588 - acc: 0.9831 - val_loss: 3.5779 - val_acc: 0.5697\n",
      "Epoch 38/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0577 - acc: 0.9800 - val_loss: 3.6683 - val_acc: 0.5617\n",
      "Epoch 39/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0904 - acc: 0.9761 - val_loss: 3.4430 - val_acc: 0.5660\n",
      "Epoch 40/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0740 - acc: 0.9782 - val_loss: 3.4370 - val_acc: 0.5746\n",
      "Epoch 41/50\n",
      "3307/3307 [==============================] - 36s 11ms/step - loss: 0.0568 - acc: 0.9873 - val_loss: 3.9038 - val_acc: 0.5482\n",
      "Epoch 42/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0624 - acc: 0.9809 - val_loss: 3.5532 - val_acc: 0.5470\n",
      "Epoch 43/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0735 - acc: 0.9816 - val_loss: 3.5690 - val_acc: 0.5592\n",
      "Epoch 44/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0508 - acc: 0.9825 - val_loss: 4.0944 - val_acc: 0.5599\n",
      "Epoch 45/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.1016 - acc: 0.9713 - val_loss: 3.4118 - val_acc: 0.5727\n",
      "Epoch 46/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0515 - acc: 0.9837 - val_loss: 3.6454 - val_acc: 0.5648\n",
      "Epoch 47/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0381 - acc: 0.9891 - val_loss: 3.6020 - val_acc: 0.5592\n",
      "Epoch 48/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0652 - acc: 0.9867 - val_loss: 3.6635 - val_acc: 0.5494\n",
      "Epoch 49/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0650 - acc: 0.9846 - val_loss: 4.0562 - val_acc: 0.5519\n",
      "Epoch 50/50\n",
      "3307/3307 [==============================] - 35s 11ms/step - loss: 0.0624 - acc: 0.9846 - val_loss: 3.5434 - val_acc: 0.5562\n"
     ]
    }
   ],
   "source": [
    "# trained model on model2 = voiced speech\n",
    "hist2 = model1.fit(voiced_feat, Y, batch_size=32, epochs=50, verbose=1, shuffle=False,\n",
    "                 validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5646163289661492 0.5887047270641069\n"
     ]
    }
   ],
   "source": [
    "acc2 = hist2.history['val_acc']\n",
    "print(np.mean(acc2), max(acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder attention function for model3\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                           input_dim=None, output_dim=None,\n",
    "                           timesteps=None, training=None):\n",
    "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
    "    # Arguments\n",
    "    x: input tensor.\n",
    "    w: weight matrix.\n",
    "    b: optional bias vector.\n",
    "    dropout: wether to apply dropout (same dropout mask\n",
    "        for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        output_dim: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "        training: training phase tensor or boolean.\n",
    "        # Returns\n",
    "            Output tensor.\n",
    "    \"\"\"\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        output_dim = K.shape(w)[1]\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "    x = K.dot(x, w)\n",
    "    if b is not None:\n",
    "        x = K.bias_add(x, b)\n",
    "    # reshape to 3D tensor\n",
    "    if K.backend() == 'tensorflow':\n",
    "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
    "        x.set_shape([None, None, output_dim])\n",
    "    else:\n",
    "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x\n",
    "    \n",
    "class AttentionDecoder(Recurrent):\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 return_probabilities=False,\n",
    "                 name='AttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states\n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
    "            \"Neural machine translation by jointly learning to align and translate.\"\n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super(AttentionDecoder, self).reset_states()\n",
    "\n",
    "        self.states = [None, None]  # y, s\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for creating the context vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the r (reset) gate\n",
    "        \"\"\"\n",
    "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_r = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_r',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for the z (update) gate\n",
    "        \"\"\"\n",
    "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_z = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_z',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the proposal\n",
    "        \"\"\"\n",
    "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_p = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_p',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for making the final prediction vector\n",
    "        \"\"\"\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # For creating the initial state:\n",
    "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='W_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "\n",
    "        return super(AttentionDecoder, self).call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, s0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        ytm, stm = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        _stm = K.repeat(stm, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        _Wxstm = K.dot(_stm, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.exp(et)\n",
    "        at_sum = K.sum(at, axis=1)\n",
    "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
    "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        # ~~~> calculate new hidden state\n",
    "        # first calculate the \"r\" gate:\n",
    "\n",
    "        rt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_r)\n",
    "            + K.dot(stm, self.U_r)\n",
    "            + K.dot(context, self.C_r)\n",
    "            + self.b_r)\n",
    "\n",
    "        # now calculate the \"z\" gate\n",
    "        zt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_z)\n",
    "            + K.dot(stm, self.U_z)\n",
    "            + K.dot(context, self.C_z)\n",
    "            + self.b_z)\n",
    "\n",
    "        # calculate the proposal hidden state:\n",
    "        s_tp = activations.tanh(\n",
    "            K.dot(ytm, self.W_p)\n",
    "            + K.dot((rt * stm), self.U_p)\n",
    "            + K.dot(context, self.C_p)\n",
    "            + self.b_p)\n",
    "\n",
    "        # new hidden state:\n",
    "        st = (1-zt)*stm + zt * s_tp\n",
    "\n",
    "        yt = activations.softmax(\n",
    "            K.dot(ytm, self.W_o)\n",
    "            + K.dot(stm, self.U_o)\n",
    "            + K.dot(context, self.C_o)\n",
    "            + self.b_o)\n",
    "\n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, st]\n",
    "        else:\n",
    "            return yt, [yt, st]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super(AttentionDecoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 100, 512)          595968    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100, 256)          1082880   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               13107712  \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 14,788,612\n",
      "Trainable params: 14,788,612\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model3 attention based\n",
    "def attention_model(optimizer='rmsprop'):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(256, return_sequences=True), input_shape=(100, 34)))\n",
    "    model.add(Bidirectional(AttentionDecoder(128, 128)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model3 = attention_model()\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3307 samples, validate on 1629 samples\n",
      "Epoch 1/50\n",
      "3307/3307 [==============================] - 114s 34ms/step - loss: 0.0942 - acc: 0.9822 - val_loss: 3.5544 - val_acc: 0.6433\n",
      "Epoch 2/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.0961 - acc: 0.9831 - val_loss: 3.4767 - val_acc: 0.6476\n",
      "Epoch 3/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.1026 - acc: 0.9813 - val_loss: 3.3323 - val_acc: 0.6341\n",
      "Epoch 4/50\n",
      "3307/3307 [==============================] - 106s 32ms/step - loss: 0.0698 - acc: 0.9855 - val_loss: 4.0059 - val_acc: 0.6390\n",
      "Epoch 5/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.1158 - acc: 0.9837 - val_loss: 4.3316 - val_acc: 0.6298\n",
      "Epoch 6/50\n",
      "3307/3307 [==============================] - 81s 24ms/step - loss: 0.1136 - acc: 0.9752 - val_loss: 3.1634 - val_acc: 0.6335\n",
      "Epoch 7/50\n",
      "3307/3307 [==============================] - 85s 26ms/step - loss: 0.0865 - acc: 0.9788 - val_loss: 3.1830 - val_acc: 0.6495\n",
      "Epoch 8/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0786 - acc: 0.9843 - val_loss: 4.5928 - val_acc: 0.6053\n",
      "Epoch 9/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0921 - acc: 0.9855 - val_loss: 3.2805 - val_acc: 0.6581\n",
      "Epoch 10/50\n",
      "3307/3307 [==============================] - 85s 26ms/step - loss: 0.1275 - acc: 0.9764 - val_loss: 2.7678 - val_acc: 0.6507\n",
      "Epoch 11/50\n",
      "3307/3307 [==============================] - 84s 26ms/step - loss: 0.1011 - acc: 0.9816 - val_loss: 3.1089 - val_acc: 0.6446\n",
      "Epoch 12/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0851 - acc: 0.9831 - val_loss: 3.3262 - val_acc: 0.6501\n",
      "Epoch 13/50\n",
      "3307/3307 [==============================] - 84s 26ms/step - loss: 0.1095 - acc: 0.9816 - val_loss: 3.4311 - val_acc: 0.6495\n",
      "Epoch 14/50\n",
      "3307/3307 [==============================] - 85s 26ms/step - loss: 0.0846 - acc: 0.9819 - val_loss: 3.2846 - val_acc: 0.6329\n",
      "Epoch 15/50\n",
      "3307/3307 [==============================] - 85s 26ms/step - loss: 0.0579 - acc: 0.9885 - val_loss: 4.0042 - val_acc: 0.6274\n",
      "Epoch 16/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.1156 - acc: 0.9794 - val_loss: 3.5317 - val_acc: 0.6366\n",
      "Epoch 17/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.1189 - acc: 0.9746 - val_loss: 3.4851 - val_acc: 0.6433\n",
      "Epoch 18/50\n",
      "3307/3307 [==============================] - 84s 26ms/step - loss: 0.0575 - acc: 0.9888 - val_loss: 3.6235 - val_acc: 0.6495\n",
      "Epoch 19/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0676 - acc: 0.9852 - val_loss: 3.5623 - val_acc: 0.6458\n",
      "Epoch 20/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0902 - acc: 0.9843 - val_loss: 3.7595 - val_acc: 0.6194\n",
      "Epoch 21/50\n",
      "3307/3307 [==============================] - 85s 26ms/step - loss: 0.1008 - acc: 0.9776 - val_loss: 3.4670 - val_acc: 0.6378\n",
      "Epoch 22/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0679 - acc: 0.9840 - val_loss: 3.3382 - val_acc: 0.6483\n",
      "Epoch 23/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0676 - acc: 0.9840 - val_loss: 3.3512 - val_acc: 0.6544\n",
      "Epoch 24/50\n",
      "3307/3307 [==============================] - 84s 26ms/step - loss: 0.0775 - acc: 0.9840 - val_loss: 3.9006 - val_acc: 0.6323\n",
      "Epoch 25/50\n",
      "3307/3307 [==============================] - 85s 26ms/step - loss: 0.0847 - acc: 0.9855 - val_loss: 3.3841 - val_acc: 0.6568\n",
      "Epoch 26/50\n",
      "3307/3307 [==============================] - 84s 26ms/step - loss: 0.0678 - acc: 0.9882 - val_loss: 3.6503 - val_acc: 0.6470\n",
      "Epoch 27/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.1130 - acc: 0.9828 - val_loss: 3.5688 - val_acc: 0.6440\n",
      "Epoch 28/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0713 - acc: 0.9852 - val_loss: 3.7175 - val_acc: 0.6440\n",
      "Epoch 29/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.1008 - acc: 0.9806 - val_loss: 3.2314 - val_acc: 0.6440\n",
      "Epoch 30/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.1237 - acc: 0.9782 - val_loss: 3.6184 - val_acc: 0.6354\n",
      "Epoch 31/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0658 - acc: 0.9876 - val_loss: 4.0872 - val_acc: 0.6249\n",
      "Epoch 32/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.1405 - acc: 0.9797 - val_loss: 3.9886 - val_acc: 0.6317\n",
      "Epoch 33/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0823 - acc: 0.9837 - val_loss: 3.8416 - val_acc: 0.6188\n",
      "Epoch 34/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.1122 - acc: 0.9785 - val_loss: 3.3470 - val_acc: 0.6366\n",
      "Epoch 35/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0901 - acc: 0.9858 - val_loss: 3.9331 - val_acc: 0.6083\n",
      "Epoch 36/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0838 - acc: 0.9852 - val_loss: 3.4855 - val_acc: 0.6255\n",
      "Epoch 37/50\n",
      "3307/3307 [==============================] - 83s 25ms/step - loss: 0.0852 - acc: 0.9876 - val_loss: 3.8032 - val_acc: 0.6255\n",
      "Epoch 38/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.1064 - acc: 0.9849 - val_loss: 3.9348 - val_acc: 0.6372\n",
      "Epoch 39/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.1241 - acc: 0.9822 - val_loss: 4.0701 - val_acc: 0.6262\n",
      "Epoch 40/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0753 - acc: 0.9867 - val_loss: 4.0182 - val_acc: 0.6262\n",
      "Epoch 41/50\n",
      "3307/3307 [==============================] - 83s 25ms/step - loss: 0.0964 - acc: 0.9819 - val_loss: 3.5902 - val_acc: 0.6280\n",
      "Epoch 42/50\n",
      "3307/3307 [==============================] - 83s 25ms/step - loss: 0.0947 - acc: 0.9855 - val_loss: 3.6392 - val_acc: 0.6298\n",
      "Epoch 43/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0823 - acc: 0.9879 - val_loss: 3.9852 - val_acc: 0.6126\n",
      "Epoch 44/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.1010 - acc: 0.9837 - val_loss: 3.8588 - val_acc: 0.6311\n",
      "Epoch 45/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.1152 - acc: 0.9791 - val_loss: 3.7925 - val_acc: 0.6274\n",
      "Epoch 46/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0526 - acc: 0.9891 - val_loss: 4.2254 - val_acc: 0.6139\n",
      "Epoch 47/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0728 - acc: 0.9846 - val_loss: 3.8169 - val_acc: 0.6433\n",
      "Epoch 48/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0834 - acc: 0.9864 - val_loss: 3.4094 - val_acc: 0.6304\n",
      "Epoch 49/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0906 - acc: 0.9858 - val_loss: 3.9761 - val_acc: 0.6139\n",
      "Epoch 50/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0992 - acc: 0.9840 - val_loss: 3.5461 - val_acc: 0.6397\n"
     ]
    }
   ],
   "source": [
    "# model3: attention decoder\n",
    "hist3 = model3.fit(x_train_speech, Y, batch_size=32, epochs=50, verbose=1, shuffle = False, \n",
    "                 validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6352977286598447 0.6580724367120648\n"
     ]
    }
   ],
   "source": [
    "acc3 = hist3.history['val_acc']\n",
    "print(np.mean(acc3), max(acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3307 samples, validate on 1629 samples\n",
      "Epoch 1/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.1650 - acc: 0.9568 - val_loss: 2.5425 - val_acc: 0.6869\n",
      "Epoch 2/50\n",
      "3307/3307 [==============================] - 83s 25ms/step - loss: 0.1056 - acc: 0.9707 - val_loss: 2.4580 - val_acc: 0.7035\n",
      "Epoch 3/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0909 - acc: 0.9749 - val_loss: 3.1722 - val_acc: 0.6734\n",
      "Epoch 4/50\n",
      "3307/3307 [==============================] - 88s 27ms/step - loss: 0.1082 - acc: 0.9779 - val_loss: 2.7613 - val_acc: 0.6826\n",
      "Epoch 5/50\n",
      "3307/3307 [==============================] - 84s 25ms/step - loss: 0.0900 - acc: 0.9809 - val_loss: 2.8511 - val_acc: 0.6986\n",
      "Epoch 6/50\n",
      "3307/3307 [==============================] - 93s 28ms/step - loss: 0.1118 - acc: 0.9710 - val_loss: 2.7711 - val_acc: 0.6808\n",
      "Epoch 7/50\n",
      "3307/3307 [==============================] - 110s 33ms/step - loss: 0.1080 - acc: 0.9755 - val_loss: 2.7550 - val_acc: 0.6746\n",
      "Epoch 8/50\n",
      "3307/3307 [==============================] - 109s 33ms/step - loss: 0.0980 - acc: 0.9770 - val_loss: 2.9389 - val_acc: 0.6710\n",
      "Epoch 9/50\n",
      "3307/3307 [==============================] - 110s 33ms/step - loss: 0.0593 - acc: 0.9888 - val_loss: 3.2677 - val_acc: 0.6679\n",
      "Epoch 10/50\n",
      "3307/3307 [==============================] - 110s 33ms/step - loss: 0.0165 - acc: 0.9967 - val_loss: 3.6531 - val_acc: 0.6777\n",
      "Epoch 11/50\n",
      "3307/3307 [==============================] - 110s 33ms/step - loss: 0.0582 - acc: 0.9879 - val_loss: 3.7539 - val_acc: 0.6599\n",
      "Epoch 12/50\n",
      "3307/3307 [==============================] - 110s 33ms/step - loss: 0.0869 - acc: 0.9813 - val_loss: 3.1041 - val_acc: 0.6789\n",
      "Epoch 13/50\n",
      "3307/3307 [==============================] - 110s 33ms/step - loss: 0.1038 - acc: 0.9788 - val_loss: 3.0807 - val_acc: 0.6746\n",
      "Epoch 14/50\n",
      "3307/3307 [==============================] - 109s 33ms/step - loss: 0.1203 - acc: 0.9758 - val_loss: 3.2896 - val_acc: 0.6753\n",
      "Epoch 15/50\n",
      "3307/3307 [==============================] - 110s 33ms/step - loss: 0.1094 - acc: 0.9785 - val_loss: 3.2961 - val_acc: 0.6642\n",
      "Epoch 16/50\n",
      "3307/3307 [==============================] - 110s 33ms/step - loss: 0.0983 - acc: 0.9813 - val_loss: 2.5981 - val_acc: 0.6716\n",
      "Epoch 17/50\n",
      "3307/3307 [==============================] - 110s 33ms/step - loss: 0.1028 - acc: 0.9794 - val_loss: 3.1077 - val_acc: 0.6796\n",
      "Epoch 18/50\n",
      "3307/3307 [==============================] - 108s 33ms/step - loss: 0.0962 - acc: 0.9813 - val_loss: 3.3049 - val_acc: 0.6673\n",
      "Epoch 19/50\n",
      "3307/3307 [==============================] - 105s 32ms/step - loss: 0.1064 - acc: 0.9776 - val_loss: 3.0187 - val_acc: 0.6667\n",
      "Epoch 20/50\n",
      "3307/3307 [==============================] - 105s 32ms/step - loss: 0.1048 - acc: 0.9758 - val_loss: 2.7582 - val_acc: 0.6575\n",
      "Epoch 21/50\n",
      "3307/3307 [==============================] - 109s 33ms/step - loss: 0.0860 - acc: 0.9800 - val_loss: 3.1352 - val_acc: 0.6679\n",
      "Epoch 22/50\n",
      "3307/3307 [==============================] - 109s 33ms/step - loss: 0.1051 - acc: 0.9785 - val_loss: 3.1989 - val_acc: 0.6746\n",
      "Epoch 23/50\n",
      "3307/3307 [==============================] - 110s 33ms/step - loss: 0.1089 - acc: 0.9764 - val_loss: 3.1817 - val_acc: 0.6618\n",
      "Epoch 24/50\n",
      "3307/3307 [==============================] - 109s 33ms/step - loss: 0.1011 - acc: 0.9758 - val_loss: 2.9025 - val_acc: 0.6703\n",
      "Epoch 25/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.0646 - acc: 0.9849 - val_loss: 3.3418 - val_acc: 0.6679\n",
      "Epoch 26/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.0435 - acc: 0.9921 - val_loss: 3.3989 - val_acc: 0.6661\n",
      "Epoch 27/50\n",
      "3307/3307 [==============================] - 115s 35ms/step - loss: 0.0700 - acc: 0.9870 - val_loss: 3.3410 - val_acc: 0.6710\n",
      "Epoch 28/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.1139 - acc: 0.9800 - val_loss: 3.3215 - val_acc: 0.6740\n",
      "Epoch 29/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.0903 - acc: 0.9803 - val_loss: 3.3892 - val_acc: 0.6722\n",
      "Epoch 30/50\n",
      "3307/3307 [==============================] - 115s 35ms/step - loss: 0.0844 - acc: 0.9822 - val_loss: 3.3936 - val_acc: 0.6575\n",
      "Epoch 31/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.1347 - acc: 0.9785 - val_loss: 3.2798 - val_acc: 0.6624\n",
      "Epoch 32/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.0786 - acc: 0.9822 - val_loss: 3.5300 - val_acc: 0.6556\n",
      "Epoch 33/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.1053 - acc: 0.9806 - val_loss: 3.0714 - val_acc: 0.6611\n",
      "Epoch 34/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.0943 - acc: 0.9809 - val_loss: 2.8791 - val_acc: 0.6384\n",
      "Epoch 35/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.0813 - acc: 0.9864 - val_loss: 3.7374 - val_acc: 0.6440\n",
      "Epoch 36/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.0818 - acc: 0.9806 - val_loss: 3.3106 - val_acc: 0.6464\n",
      "Epoch 37/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.0853 - acc: 0.9800 - val_loss: 3.3756 - val_acc: 0.6740\n",
      "Epoch 38/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.0680 - acc: 0.9849 - val_loss: 3.4239 - val_acc: 0.6476\n",
      "Epoch 39/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.0737 - acc: 0.9861 - val_loss: 3.7994 - val_acc: 0.6409\n",
      "Epoch 40/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.0943 - acc: 0.9819 - val_loss: 3.4728 - val_acc: 0.6507\n",
      "Epoch 41/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.1176 - acc: 0.9806 - val_loss: 3.3175 - val_acc: 0.6593\n",
      "Epoch 42/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.0837 - acc: 0.9828 - val_loss: 3.4675 - val_acc: 0.6642\n",
      "Epoch 43/50\n",
      "3307/3307 [==============================] - 117s 35ms/step - loss: 0.1065 - acc: 0.9800 - val_loss: 3.0640 - val_acc: 0.6483\n",
      "Epoch 44/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.1071 - acc: 0.9764 - val_loss: 3.0013 - val_acc: 0.6599\n",
      "Epoch 45/50\n",
      "3307/3307 [==============================] - 118s 36ms/step - loss: 0.0912 - acc: 0.9809 - val_loss: 3.2576 - val_acc: 0.6611\n",
      "Epoch 46/50\n",
      "3307/3307 [==============================] - 117s 35ms/step - loss: 0.1194 - acc: 0.9803 - val_loss: 3.5747 - val_acc: 0.6661\n",
      "Epoch 47/50\n",
      "3307/3307 [==============================] - 117s 35ms/step - loss: 0.1143 - acc: 0.9782 - val_loss: 3.0141 - val_acc: 0.6624\n",
      "Epoch 48/50\n",
      "3307/3307 [==============================] - 117s 35ms/step - loss: 0.0782 - acc: 0.9825 - val_loss: 3.2621 - val_acc: 0.6464\n",
      "Epoch 49/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.0668 - acc: 0.9867 - val_loss: 3.8189 - val_acc: 0.6421\n",
      "Epoch 50/50\n",
      "3307/3307 [==============================] - 116s 35ms/step - loss: 0.0867 - acc: 0.9834 - val_loss: 3.5786 - val_acc: 0.6464\n"
     ]
    }
   ],
   "source": [
    "# model4: voiced + attention decoder\n",
    "hist4 = model3.fit(voiced_feat, Y, batch_size=32, epochs=50, verbose=1, shuffle=False, \n",
    "                 validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6654634744380792 0.7034990790982126\n"
     ]
    }
   ],
   "source": [
    "acc4 = hist4.history['val_acc']\n",
    "print(np.mean(acc4), max(acc4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4936, 100, 34)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Effect of minimum sample of noise removal on performance, try minimum duration 0.06 and 0.1 s\n",
    "# read feature data mindur = 0.06 s\n",
    "feat_data2 = np.load('voiced_feat_file_006_001.npy')\n",
    "feat_data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3307 samples, validate on 1629 samples\n",
      "Epoch 1/50\n",
      "3307/3307 [==============================] - 70s 21ms/step - loss: 0.1069 - acc: 0.9822 - val_loss: 3.5604 - val_acc: 0.6317\n",
      "Epoch 2/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.1208 - acc: 0.9767 - val_loss: 3.1562 - val_acc: 0.6384\n",
      "Epoch 3/50\n",
      "3307/3307 [==============================] - 70s 21ms/step - loss: 0.0693 - acc: 0.9867 - val_loss: 3.8230 - val_acc: 0.6139\n",
      "Epoch 4/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0860 - acc: 0.9855 - val_loss: 3.7134 - val_acc: 0.6440\n",
      "Epoch 5/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0738 - acc: 0.9831 - val_loss: 3.3179 - val_acc: 0.6274\n",
      "Epoch 6/50\n",
      "3307/3307 [==============================] - 70s 21ms/step - loss: 0.0779 - acc: 0.9834 - val_loss: 3.6616 - val_acc: 0.6255\n",
      "Epoch 7/50\n",
      "3307/3307 [==============================] - 75s 23ms/step - loss: 0.0594 - acc: 0.9891 - val_loss: 4.0713 - val_acc: 0.6255\n",
      "Epoch 8/50\n",
      "3307/3307 [==============================] - 75s 23ms/step - loss: 0.0537 - acc: 0.9903 - val_loss: 4.4563 - val_acc: 0.5991\n",
      "Epoch 9/50\n",
      "3307/3307 [==============================] - 74s 22ms/step - loss: 0.1028 - acc: 0.9828 - val_loss: 4.1645 - val_acc: 0.6028\n",
      "Epoch 10/50\n",
      "3307/3307 [==============================] - 73s 22ms/step - loss: 0.0775 - acc: 0.9846 - val_loss: 3.9083 - val_acc: 0.6354\n",
      "Epoch 11/50\n",
      "3307/3307 [==============================] - 71s 21ms/step - loss: 0.0885 - acc: 0.9861 - val_loss: 4.2681 - val_acc: 0.6151\n",
      "Epoch 12/50\n",
      "3307/3307 [==============================] - 72s 22ms/step - loss: 0.0765 - acc: 0.9849 - val_loss: 3.5844 - val_acc: 0.6372\n",
      "Epoch 13/50\n",
      "3307/3307 [==============================] - 71s 21ms/step - loss: 0.0816 - acc: 0.9843 - val_loss: 3.5792 - val_acc: 0.6298\n",
      "Epoch 14/50\n",
      "3307/3307 [==============================] - 71s 21ms/step - loss: 0.0551 - acc: 0.9879 - val_loss: 4.0868 - val_acc: 0.6323\n",
      "Epoch 15/50\n",
      "3307/3307 [==============================] - 70s 21ms/step - loss: 0.1145 - acc: 0.9819 - val_loss: 3.6705 - val_acc: 0.6347\n",
      "Epoch 16/50\n",
      "3307/3307 [==============================] - 71s 21ms/step - loss: 0.0879 - acc: 0.9849 - val_loss: 3.3948 - val_acc: 0.6366\n",
      "Epoch 17/50\n",
      "3307/3307 [==============================] - 70s 21ms/step - loss: 0.0855 - acc: 0.9831 - val_loss: 3.6072 - val_acc: 0.6317\n",
      "Epoch 18/50\n",
      "3307/3307 [==============================] - 78s 24ms/step - loss: 0.0786 - acc: 0.9837 - val_loss: 3.1714 - val_acc: 0.6317\n",
      "Epoch 19/50\n",
      "3307/3307 [==============================] - 71s 22ms/step - loss: 0.0867 - acc: 0.9840 - val_loss: 3.4192 - val_acc: 0.6169\n",
      "Epoch 20/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.0750 - acc: 0.9840 - val_loss: 3.5124 - val_acc: 0.6212\n",
      "Epoch 21/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0752 - acc: 0.9858 - val_loss: 3.5160 - val_acc: 0.6182\n",
      "Epoch 22/50\n",
      "3307/3307 [==============================] - 74s 22ms/step - loss: 0.0591 - acc: 0.9894 - val_loss: 3.7501 - val_acc: 0.6206\n",
      "Epoch 23/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.0507 - acc: 0.9900 - val_loss: 3.5411 - val_acc: 0.6249\n",
      "Epoch 24/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.0665 - acc: 0.9843 - val_loss: 3.9880 - val_acc: 0.6145\n",
      "Epoch 25/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.0508 - acc: 0.9888 - val_loss: 3.6306 - val_acc: 0.6452\n",
      "Epoch 26/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.0506 - acc: 0.9906 - val_loss: 3.8469 - val_acc: 0.6182\n",
      "Epoch 27/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.0531 - acc: 0.9927 - val_loss: 3.9629 - val_acc: 0.6194\n",
      "Epoch 28/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.0537 - acc: 0.9888 - val_loss: 3.8748 - val_acc: 0.6255\n",
      "Epoch 29/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.0596 - acc: 0.9891 - val_loss: 3.7102 - val_acc: 0.6120\n",
      "Epoch 30/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.0554 - acc: 0.9900 - val_loss: 3.9115 - val_acc: 0.6139\n",
      "Epoch 31/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.1028 - acc: 0.9861 - val_loss: 3.8618 - val_acc: 0.6225\n",
      "Epoch 32/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.0397 - acc: 0.9906 - val_loss: 4.1686 - val_acc: 0.6108\n",
      "Epoch 33/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.0683 - acc: 0.9879 - val_loss: 4.4501 - val_acc: 0.5973\n",
      "Epoch 34/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.1014 - acc: 0.9840 - val_loss: 4.1188 - val_acc: 0.6059\n",
      "Epoch 35/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.0661 - acc: 0.9891 - val_loss: 3.8112 - val_acc: 0.6047\n",
      "Epoch 36/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.0783 - acc: 0.9849 - val_loss: 3.9915 - val_acc: 0.6096\n",
      "Epoch 37/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.0604 - acc: 0.9882 - val_loss: 3.9721 - val_acc: 0.6022\n",
      "Epoch 38/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.0416 - acc: 0.9933 - val_loss: 4.2377 - val_acc: 0.6114\n",
      "Epoch 39/50\n",
      "3307/3307 [==============================] - 74s 23ms/step - loss: 0.0759 - acc: 0.9876 - val_loss: 4.0261 - val_acc: 0.6231\n",
      "Epoch 40/50\n",
      "3307/3307 [==============================] - 75s 23ms/step - loss: 0.0940 - acc: 0.9858 - val_loss: 3.8571 - val_acc: 0.5973\n",
      "Epoch 41/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0976 - acc: 0.9825 - val_loss: 3.4895 - val_acc: 0.6212\n",
      "Epoch 42/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0581 - acc: 0.9885 - val_loss: 3.8418 - val_acc: 0.6347\n",
      "Epoch 43/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0981 - acc: 0.9870 - val_loss: 3.9737 - val_acc: 0.6255\n",
      "Epoch 44/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.1119 - acc: 0.9846 - val_loss: 3.6330 - val_acc: 0.6243\n",
      "Epoch 45/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.0974 - acc: 0.9834 - val_loss: 3.5111 - val_acc: 0.6169\n",
      "Epoch 46/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0783 - acc: 0.9873 - val_loss: 4.3753 - val_acc: 0.6157\n",
      "Epoch 47/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0945 - acc: 0.9840 - val_loss: 3.9206 - val_acc: 0.6108\n",
      "Epoch 48/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0921 - acc: 0.9867 - val_loss: 3.8180 - val_acc: 0.6176\n",
      "Epoch 49/50\n",
      "3307/3307 [==============================] - 78s 23ms/step - loss: 0.0768 - acc: 0.9888 - val_loss: 3.6377 - val_acc: 0.6274\n",
      "Epoch 50/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0554 - acc: 0.9876 - val_loss: 3.9288 - val_acc: 0.6182\n"
     ]
    }
   ],
   "source": [
    "hist5 = model3.fit(feat_data2, Y, batch_size=32, epochs=50, verbose=1, shuffle=False,\n",
    "                  validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6208103130451369 0.6451810927314946\n"
     ]
    }
   ],
   "source": [
    "acc5 = hist5.history['val_acc']\n",
    "print(np.mean(acc5), max(acc5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4936, 100, 34)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read feature data, mindur = 0.1 s\n",
    "feat_data3 = np.load('voiced_feat_file_01_001.npy')\n",
    "feat_data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3307 samples, validate on 1629 samples\n",
      "Epoch 1/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0795 - acc: 0.9858 - val_loss: 3.9436 - val_acc: 0.6225\n",
      "Epoch 2/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.0669 - acc: 0.9882 - val_loss: 3.6074 - val_acc: 0.6176\n",
      "Epoch 3/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0846 - acc: 0.9867 - val_loss: 3.9433 - val_acc: 0.6194\n",
      "Epoch 4/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.1031 - acc: 0.9873 - val_loss: 4.0210 - val_acc: 0.6090\n",
      "Epoch 5/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.1543 - acc: 0.9791 - val_loss: 3.8535 - val_acc: 0.5955\n",
      "Epoch 6/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0719 - acc: 0.9894 - val_loss: 4.1557 - val_acc: 0.6163\n",
      "Epoch 7/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0652 - acc: 0.9891 - val_loss: 3.9556 - val_acc: 0.6163\n",
      "Epoch 8/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.1179 - acc: 0.9797 - val_loss: 4.1320 - val_acc: 0.6022\n",
      "Epoch 9/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0748 - acc: 0.9879 - val_loss: 3.7304 - val_acc: 0.6292\n",
      "Epoch 10/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0732 - acc: 0.9876 - val_loss: 3.8388 - val_acc: 0.6133\n",
      "Epoch 11/50\n",
      "3307/3307 [==============================] - 74s 23ms/step - loss: 0.0889 - acc: 0.9879 - val_loss: 4.0606 - val_acc: 0.6010\n",
      "Epoch 12/50\n",
      "3307/3307 [==============================] - 79s 24ms/step - loss: 0.0734 - acc: 0.9855 - val_loss: 4.1299 - val_acc: 0.6096\n",
      "Epoch 13/50\n",
      "3307/3307 [==============================] - 78s 24ms/step - loss: 0.0784 - acc: 0.9828 - val_loss: 4.0828 - val_acc: 0.6041\n",
      "Epoch 14/50\n",
      "3307/3307 [==============================] - 74s 22ms/step - loss: 0.0728 - acc: 0.9873 - val_loss: 4.2354 - val_acc: 0.5936\n",
      "Epoch 15/50\n",
      "3307/3307 [==============================] - 78s 24ms/step - loss: 0.0762 - acc: 0.9840 - val_loss: 4.1238 - val_acc: 0.6219\n",
      "Epoch 16/50\n",
      "3307/3307 [==============================] - 78s 24ms/step - loss: 0.0634 - acc: 0.9894 - val_loss: 3.9961 - val_acc: 0.6219\n",
      "Epoch 17/50\n",
      "3307/3307 [==============================] - 75s 23ms/step - loss: 0.0992 - acc: 0.9843 - val_loss: 4.1242 - val_acc: 0.6059\n",
      "Epoch 18/50\n",
      "3307/3307 [==============================] - 78s 24ms/step - loss: 0.0598 - acc: 0.9870 - val_loss: 3.9727 - val_acc: 0.6034\n",
      "Epoch 19/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0668 - acc: 0.9870 - val_loss: 4.0103 - val_acc: 0.6212\n",
      "Epoch 20/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0574 - acc: 0.9897 - val_loss: 4.3440 - val_acc: 0.5887\n",
      "Epoch 21/50\n",
      "3307/3307 [==============================] - 78s 23ms/step - loss: 0.0968 - acc: 0.9852 - val_loss: 3.8046 - val_acc: 0.6041\n",
      "Epoch 22/50\n",
      "3307/3307 [==============================] - 75s 23ms/step - loss: 0.1337 - acc: 0.9822 - val_loss: 4.1273 - val_acc: 0.6034\n",
      "Epoch 23/50\n",
      "3307/3307 [==============================] - 70s 21ms/step - loss: 0.1018 - acc: 0.9858 - val_loss: 4.2216 - val_acc: 0.6114\n",
      "Epoch 24/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0753 - acc: 0.9879 - val_loss: 4.4915 - val_acc: 0.5905\n",
      "Epoch 25/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0877 - acc: 0.9837 - val_loss: 4.0047 - val_acc: 0.5961\n",
      "Epoch 26/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0773 - acc: 0.9852 - val_loss: 4.0784 - val_acc: 0.6053\n",
      "Epoch 27/50\n",
      "3307/3307 [==============================] - 70s 21ms/step - loss: 0.0365 - acc: 0.9936 - val_loss: 4.2977 - val_acc: 0.6083\n",
      "Epoch 28/50\n",
      "3307/3307 [==============================] - 70s 21ms/step - loss: 0.0563 - acc: 0.9888 - val_loss: 4.0005 - val_acc: 0.6083\n",
      "Epoch 29/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0557 - acc: 0.9915 - val_loss: 4.4879 - val_acc: 0.6145\n",
      "Epoch 30/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0857 - acc: 0.9849 - val_loss: 4.2560 - val_acc: 0.6120\n",
      "Epoch 31/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0603 - acc: 0.9885 - val_loss: 3.9937 - val_acc: 0.6034\n",
      "Epoch 32/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0691 - acc: 0.9837 - val_loss: 4.0920 - val_acc: 0.6010\n",
      "Epoch 33/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0667 - acc: 0.9891 - val_loss: 4.5352 - val_acc: 0.6022\n",
      "Epoch 34/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0963 - acc: 0.9873 - val_loss: 4.3458 - val_acc: 0.6010\n",
      "Epoch 35/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0802 - acc: 0.9891 - val_loss: 4.0836 - val_acc: 0.6114\n",
      "Epoch 36/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.1057 - acc: 0.9831 - val_loss: 4.4360 - val_acc: 0.5991\n",
      "Epoch 37/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0918 - acc: 0.9846 - val_loss: 3.9552 - val_acc: 0.6028\n",
      "Epoch 38/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0532 - acc: 0.9888 - val_loss: 4.3097 - val_acc: 0.6028\n",
      "Epoch 39/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0780 - acc: 0.9864 - val_loss: 4.2152 - val_acc: 0.5936\n",
      "Epoch 40/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0787 - acc: 0.9861 - val_loss: 4.1198 - val_acc: 0.5973\n",
      "Epoch 41/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0967 - acc: 0.9849 - val_loss: 4.3872 - val_acc: 0.6176\n",
      "Epoch 42/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0579 - acc: 0.9894 - val_loss: 4.2391 - val_acc: 0.5862\n",
      "Epoch 43/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0645 - acc: 0.9879 - val_loss: 4.0497 - val_acc: 0.5967\n",
      "Epoch 44/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0778 - acc: 0.9870 - val_loss: 3.9660 - val_acc: 0.6120\n",
      "Epoch 45/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0799 - acc: 0.9855 - val_loss: 5.0122 - val_acc: 0.5948\n",
      "Epoch 46/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0809 - acc: 0.9843 - val_loss: 4.6378 - val_acc: 0.5820\n",
      "Epoch 47/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0503 - acc: 0.9906 - val_loss: 4.5570 - val_acc: 0.5813\n",
      "Epoch 48/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0815 - acc: 0.9849 - val_loss: 4.7438 - val_acc: 0.5893\n",
      "Epoch 49/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0644 - acc: 0.9915 - val_loss: 4.3850 - val_acc: 0.6022\n",
      "Epoch 50/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.0845 - acc: 0.9879 - val_loss: 4.3851 - val_acc: 0.5998\n"
     ]
    }
   ],
   "source": [
    "hist6 = model3.fit(feat_data3, Y, batch_size=32, epochs=50, verbose=1, shuffle=False,\n",
    "                  validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6048618784248646 0.6292203805192692\n"
     ]
    }
   ],
   "source": [
    "acc6 = hist6.history['val_acc']\n",
    "print(np.mean(acc6), max(acc6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4936, 100, 34)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now change the threshold for silence removal, use the same minimum duration silence (0.01)\n",
    "# read feature data, th = 0.007 s\n",
    "feat_data4 = np.load('voiced_feat_file_001_007.npy')\n",
    "feat_data4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3307 samples, validate on 1629 samples\n",
      "Epoch 1/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.6967 - acc: 0.8818 - val_loss: 2.9832 - val_acc: 0.5537\n",
      "Epoch 2/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.4937 - acc: 0.9026 - val_loss: 2.6333 - val_acc: 0.5599\n",
      "Epoch 3/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.4133 - acc: 0.9138 - val_loss: 2.7801 - val_acc: 0.5820\n",
      "Epoch 4/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.3315 - acc: 0.9289 - val_loss: 2.8622 - val_acc: 0.5666\n",
      "Epoch 5/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.2943 - acc: 0.9395 - val_loss: 2.9996 - val_acc: 0.5740\n",
      "Epoch 6/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.2378 - acc: 0.9483 - val_loss: 3.0869 - val_acc: 0.5648\n",
      "Epoch 7/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.3256 - acc: 0.9353 - val_loss: 3.3222 - val_acc: 0.5537\n",
      "Epoch 8/50\n",
      "3307/3307 [==============================] - 69s 21ms/step - loss: 0.2751 - acc: 0.9477 - val_loss: 3.1896 - val_acc: 0.5691\n",
      "Epoch 9/50\n",
      "3307/3307 [==============================] - 70s 21ms/step - loss: 0.2185 - acc: 0.9540 - val_loss: 3.2969 - val_acc: 0.5599\n",
      "Epoch 10/50\n",
      "3307/3307 [==============================] - 75s 23ms/step - loss: 0.2041 - acc: 0.9649 - val_loss: 3.6779 - val_acc: 0.5605\n",
      "Epoch 11/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.1705 - acc: 0.9631 - val_loss: 3.1764 - val_acc: 0.5697\n",
      "Epoch 12/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.1742 - acc: 0.9673 - val_loss: 3.4559 - val_acc: 0.5721\n",
      "Epoch 13/50\n",
      "3307/3307 [==============================] - 75s 23ms/step - loss: 0.1664 - acc: 0.9616 - val_loss: 3.6280 - val_acc: 0.5691\n",
      "Epoch 14/50\n",
      "3307/3307 [==============================] - 75s 23ms/step - loss: 0.1446 - acc: 0.9740 - val_loss: 3.6273 - val_acc: 0.5727\n",
      "Epoch 15/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.1603 - acc: 0.9695 - val_loss: 3.9300 - val_acc: 0.5543\n",
      "Epoch 16/50\n",
      "3307/3307 [==============================] - 74s 22ms/step - loss: 0.1492 - acc: 0.9728 - val_loss: 4.1889 - val_acc: 0.5482\n",
      "Epoch 17/50\n",
      "3307/3307 [==============================] - 75s 23ms/step - loss: 0.1658 - acc: 0.9698 - val_loss: 3.7074 - val_acc: 0.5795\n",
      "Epoch 18/50\n",
      "3307/3307 [==============================] - 75s 23ms/step - loss: 0.1547 - acc: 0.9788 - val_loss: 3.8258 - val_acc: 0.5611\n",
      "Epoch 19/50\n",
      "3307/3307 [==============================] - 75s 23ms/step - loss: 0.1423 - acc: 0.9782 - val_loss: 3.8435 - val_acc: 0.5758\n",
      "Epoch 20/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.1180 - acc: 0.9767 - val_loss: 4.1079 - val_acc: 0.5635\n",
      "Epoch 21/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.1099 - acc: 0.9834 - val_loss: 4.7423 - val_acc: 0.5457\n",
      "Epoch 22/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.1212 - acc: 0.9806 - val_loss: 4.1102 - val_acc: 0.5666\n",
      "Epoch 23/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.1379 - acc: 0.9737 - val_loss: 4.0807 - val_acc: 0.5654\n",
      "Epoch 24/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.1342 - acc: 0.9794 - val_loss: 3.9719 - val_acc: 0.5549\n",
      "Epoch 25/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.1466 - acc: 0.9761 - val_loss: 4.2331 - val_acc: 0.5457\n",
      "Epoch 26/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.1739 - acc: 0.9716 - val_loss: 3.9088 - val_acc: 0.5525\n",
      "Epoch 27/50\n",
      "3307/3307 [==============================] - 78s 24ms/step - loss: 0.1489 - acc: 0.9764 - val_loss: 3.8459 - val_acc: 0.5715\n",
      "Epoch 28/50\n",
      "3307/3307 [==============================] - 75s 23ms/step - loss: 0.1266 - acc: 0.9770 - val_loss: 4.4829 - val_acc: 0.5672\n",
      "Epoch 29/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.1424 - acc: 0.9755 - val_loss: 3.9846 - val_acc: 0.5740\n",
      "Epoch 30/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.1369 - acc: 0.9767 - val_loss: 4.1444 - val_acc: 0.5648\n",
      "Epoch 31/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.1180 - acc: 0.9791 - val_loss: 3.9132 - val_acc: 0.5611\n",
      "Epoch 32/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0723 - acc: 0.9806 - val_loss: 4.1520 - val_acc: 0.5869\n",
      "Epoch 33/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.1469 - acc: 0.9761 - val_loss: 3.9486 - val_acc: 0.5838\n",
      "Epoch 34/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.1212 - acc: 0.9773 - val_loss: 4.4323 - val_acc: 0.5844\n",
      "Epoch 35/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0740 - acc: 0.9849 - val_loss: 4.2161 - val_acc: 0.5869\n",
      "Epoch 36/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0643 - acc: 0.9885 - val_loss: 4.2661 - val_acc: 0.5721\n",
      "Epoch 37/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0991 - acc: 0.9834 - val_loss: 4.4043 - val_acc: 0.5433\n",
      "Epoch 38/50\n",
      "3307/3307 [==============================] - 78s 24ms/step - loss: 0.0883 - acc: 0.9840 - val_loss: 4.4923 - val_acc: 0.5709\n",
      "Epoch 39/50\n",
      "3307/3307 [==============================] - 73s 22ms/step - loss: 0.1278 - acc: 0.9764 - val_loss: 4.0299 - val_acc: 0.5666\n",
      "Epoch 40/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.1400 - acc: 0.9776 - val_loss: 3.7782 - val_acc: 0.5654\n",
      "Epoch 41/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.1064 - acc: 0.9840 - val_loss: 4.5742 - val_acc: 0.5648\n",
      "Epoch 42/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.1141 - acc: 0.9816 - val_loss: 4.6556 - val_acc: 0.5605\n",
      "Epoch 43/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.1133 - acc: 0.9834 - val_loss: 4.5203 - val_acc: 0.5641\n",
      "Epoch 44/50\n",
      "3307/3307 [==============================] - 76s 23ms/step - loss: 0.1405 - acc: 0.9767 - val_loss: 4.5290 - val_acc: 0.5691\n",
      "Epoch 45/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.0982 - acc: 0.9852 - val_loss: 4.3558 - val_acc: 0.5654\n",
      "Epoch 46/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.1060 - acc: 0.9797 - val_loss: 4.5116 - val_acc: 0.5500\n",
      "Epoch 47/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.1442 - acc: 0.9806 - val_loss: 4.9033 - val_acc: 0.5703\n",
      "Epoch 48/50\n",
      "3307/3307 [==============================] - 77s 23ms/step - loss: 0.1201 - acc: 0.9794 - val_loss: 4.1098 - val_acc: 0.5605\n",
      "Epoch 49/50\n",
      "3307/3307 [==============================] - 74s 22ms/step - loss: 0.0806 - acc: 0.9855 - val_loss: 4.7623 - val_acc: 0.5384\n",
      "Epoch 50/50\n",
      "3307/3307 [==============================] - 71s 22ms/step - loss: 0.0936 - acc: 0.9837 - val_loss: 4.1604 - val_acc: 0.5537\n"
     ]
    }
   ],
   "source": [
    "hist7 = model3.fit(feat_data4, Y, batch_size=32, epochs=50, verbose=1, shuffle=False,\n",
    "                 validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5647268262232937 0.5868631061086484\n"
     ]
    }
   ],
   "source": [
    "acc7 = hist7.history['val_acc']\n",
    "print(np.mean(acc7), max(acc7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4810, 100, 34)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read feature data, th = 0.01 s\n",
    "feat_data5 = np.load('voiced_feat_file_001_01.npy')\n",
    "feat_data5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4810, 4)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in this case we need to remove some label data because those data below 0.1s threshold (noise dominant)\n",
    "emo = np.delete(Y, (1061, 1430, 1500, 1552, 1566, 1574, 1575, 1576, 1862, 1863, 1864, 1865, 1868, 1869,\n",
    "                    1875, 1878, 1880, 1883, 1884, 1886, 1888, 1890, 1892, 1893, 1930, 1931, 1932, 1969,\n",
    "                    1970, 1971, 1975, 1976, 1977, 1979, 1980, 1981, 1984, 1985, 1986, 1987, 1988, 1989, \n",
    "                    1990, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2002, 2003, 2076, 2106, 2110,\n",
    "                    2177, 2178, 2179, 2180, 2206, 2241, 2242, 2243, 2245, 2246, 2253, 2254, 2262, 2263, \n",
    "                    2357, 2358, 2359, 2362, 2368, 2373, 2374, 2418, 2523, 2525, 2526, 2534, 2539, 2542,\n",
    "                    2549, 2552, 2553, 2554, 2555, 2556, 2561, 2562, 2563, 2564, 2578, 2670, 2671, 2672, \n",
    "                    2692, 2694, 2695, 2728, 2733, 2889, 2890, 3034, 3304, 3511, 3524, 3525, 3528, 3655, \n",
    "                    3802, 3864, 3930, 4038, 4049, 4051, 4061, 4193, 4241, 4301, 4302, 4307, 4569, 4570), 0)\n",
    "emo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3222 samples, validate on 1588 samples\n",
      "Epoch 1/50\n",
      "3222/3222 [==============================] - 68s 21ms/step - loss: 0.8262 - acc: 0.8240 - val_loss: 2.1766 - val_acc: 0.4767\n",
      "Epoch 2/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.5702 - acc: 0.8482 - val_loss: 2.2685 - val_acc: 0.4855\n",
      "Epoch 3/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.4322 - acc: 0.8908 - val_loss: 2.3117 - val_acc: 0.4887\n",
      "Epoch 4/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.3533 - acc: 0.8994 - val_loss: 2.8748 - val_acc: 0.4994\n",
      "Epoch 5/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.2992 - acc: 0.9165 - val_loss: 2.5834 - val_acc: 0.5057\n",
      "Epoch 6/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.2485 - acc: 0.9286 - val_loss: 3.0098 - val_acc: 0.5113\n",
      "Epoch 7/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.2334 - acc: 0.9460 - val_loss: 3.0300 - val_acc: 0.5157\n",
      "Epoch 8/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.2224 - acc: 0.9472 - val_loss: 3.3612 - val_acc: 0.5113\n",
      "Epoch 9/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.2129 - acc: 0.9466 - val_loss: 3.3315 - val_acc: 0.5063\n",
      "Epoch 10/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1834 - acc: 0.9584 - val_loss: 3.9049 - val_acc: 0.5057\n",
      "Epoch 11/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1799 - acc: 0.9612 - val_loss: 3.7115 - val_acc: 0.4962\n",
      "Epoch 12/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1589 - acc: 0.9624 - val_loss: 3.5717 - val_acc: 0.5189\n",
      "Epoch 13/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1542 - acc: 0.9649 - val_loss: 3.5453 - val_acc: 0.5189\n",
      "Epoch 14/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1423 - acc: 0.9683 - val_loss: 3.9696 - val_acc: 0.5050\n",
      "Epoch 15/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1519 - acc: 0.9690 - val_loss: 4.2025 - val_acc: 0.4969\n",
      "Epoch 16/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1240 - acc: 0.9721 - val_loss: 4.2629 - val_acc: 0.4931\n",
      "Epoch 17/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1208 - acc: 0.9758 - val_loss: 3.9719 - val_acc: 0.5076\n",
      "Epoch 18/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1534 - acc: 0.9646 - val_loss: 4.0869 - val_acc: 0.4969\n",
      "Epoch 19/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1022 - acc: 0.9718 - val_loss: 3.8781 - val_acc: 0.5113\n",
      "Epoch 20/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1360 - acc: 0.9721 - val_loss: 4.1484 - val_acc: 0.4918\n",
      "Epoch 21/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1250 - acc: 0.9730 - val_loss: 4.5445 - val_acc: 0.5132\n",
      "Epoch 22/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1783 - acc: 0.9718 - val_loss: 4.2850 - val_acc: 0.5025\n",
      "Epoch 23/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1130 - acc: 0.9749 - val_loss: 4.3565 - val_acc: 0.5082\n",
      "Epoch 24/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1310 - acc: 0.9727 - val_loss: 4.1231 - val_acc: 0.4994\n",
      "Epoch 25/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1481 - acc: 0.9736 - val_loss: 4.5040 - val_acc: 0.4887\n",
      "Epoch 26/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1036 - acc: 0.9761 - val_loss: 4.5601 - val_acc: 0.5082\n",
      "Epoch 27/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.0981 - acc: 0.9770 - val_loss: 4.7716 - val_acc: 0.5157\n",
      "Epoch 28/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1035 - acc: 0.9804 - val_loss: 4.5283 - val_acc: 0.5069\n",
      "Epoch 29/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.0933 - acc: 0.9804 - val_loss: 4.1930 - val_acc: 0.5094\n",
      "Epoch 30/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1158 - acc: 0.9780 - val_loss: 4.7809 - val_acc: 0.5000\n",
      "Epoch 31/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.0889 - acc: 0.9839 - val_loss: 4.8428 - val_acc: 0.5063\n",
      "Epoch 32/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1266 - acc: 0.9767 - val_loss: 4.7446 - val_acc: 0.5025\n",
      "Epoch 33/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.0977 - acc: 0.9801 - val_loss: 4.4680 - val_acc: 0.5094\n",
      "Epoch 34/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1406 - acc: 0.9730 - val_loss: 4.3891 - val_acc: 0.4943\n",
      "Epoch 35/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.0921 - acc: 0.9792 - val_loss: 4.6395 - val_acc: 0.4956\n",
      "Epoch 36/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1160 - acc: 0.9767 - val_loss: 4.1463 - val_acc: 0.4893\n",
      "Epoch 37/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1216 - acc: 0.9736 - val_loss: 4.6710 - val_acc: 0.4899\n",
      "Epoch 38/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.0783 - acc: 0.9857 - val_loss: 4.8900 - val_acc: 0.4975\n",
      "Epoch 39/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1313 - acc: 0.9736 - val_loss: 4.7093 - val_acc: 0.4893\n",
      "Epoch 40/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.0932 - acc: 0.9808 - val_loss: 4.7475 - val_acc: 0.4861\n",
      "Epoch 41/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1054 - acc: 0.9777 - val_loss: 4.5856 - val_acc: 0.5139\n",
      "Epoch 42/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.0913 - acc: 0.9792 - val_loss: 4.7949 - val_acc: 0.5113\n",
      "Epoch 43/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.0656 - acc: 0.9854 - val_loss: 4.9189 - val_acc: 0.5107\n",
      "Epoch 44/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.0910 - acc: 0.9820 - val_loss: 5.1448 - val_acc: 0.5038\n",
      "Epoch 45/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1208 - acc: 0.9786 - val_loss: 4.7240 - val_acc: 0.5031\n",
      "Epoch 46/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.0908 - acc: 0.9811 - val_loss: 5.1464 - val_acc: 0.4981\n",
      "Epoch 47/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1166 - acc: 0.9795 - val_loss: 4.7623 - val_acc: 0.4906\n",
      "Epoch 48/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1452 - acc: 0.9773 - val_loss: 4.4681 - val_acc: 0.4987\n",
      "Epoch 49/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.1165 - acc: 0.9773 - val_loss: 4.7900 - val_acc: 0.5063\n",
      "Epoch 50/50\n",
      "3222/3222 [==============================] - 67s 21ms/step - loss: 0.0534 - acc: 0.9919 - val_loss: 5.2278 - val_acc: 0.5183\n"
     ]
    }
   ],
   "source": [
    "hist8 = model3.fit(feat_data5, emo, batch_size=32, epochs=50, verbose=1, shuffle=False,\n",
    "                 validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5022040302537251 0.5188916877324996\n"
     ]
    }
   ],
   "source": [
    "acc8 = hist8.history['val_acc']\n",
    "print(np.mean(acc8), max(acc8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
