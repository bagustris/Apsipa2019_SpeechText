{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import wave\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import LSTM, Input, Flatten, Add, concatenate, Embedding, Convolution1D, Dropout, Dense, merge, Bidirectional\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras import optimizers\n",
    "\n",
    "from features import *\n",
    "from helper import *\n",
    "from attention_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "emotions_used = np.array(['ang', 'exc', 'neu', 'sad'])\n",
    "data_path = '/media/bagus/data01/dataset/IEMOCAP_full_release/'\n",
    "sessions = ['Session1', 'Session2', 'Session3', 'Session4', 'Session5']\n",
    "framerate = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(data_path +'data_collected.pickle', 'rb') as handle:\n",
    "    data2 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "\n",
    "for ses_mod in data2:\n",
    "    text.append(ses_mod['transcription'])\n",
    "    \n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "token_tr_X = tokenizer.texts_to_sequences(text)\n",
    "x_train_text = []\n",
    "\n",
    "x_train_text = sequence.pad_sequences(token_tr_X, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2736 unique tokens\n",
      "../../data/glove.840B.300d.txt\n",
      "G Word embeddings: 2196018\n",
      "G Null word embeddings: 100\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "#file_loc = data_path + '../glove.42B.300d.txt'\n",
    "file_loc = '../../data/glove.840B.300d.txt'\n",
    "print (file_loc)\n",
    "\n",
    "gembeddings_index = {}\n",
    "with codecs.open(file_loc, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        gembedding = np.asarray(values[1:], dtype='float32')\n",
    "        gembeddings_index[word] = gembedding\n",
    "#\n",
    "f.close()\n",
    "print('G Word embeddings:', len(gembeddings_index))\n",
    "\n",
    "nb_words = len(word_index) +1\n",
    "g_word_embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    gembedding_vector = gembeddings_index.get(word)\n",
    "    if gembedding_vector is not None:\n",
    "        g_word_embedding_matrix[i] = gembedding_vector\n",
    "        \n",
    "print('G Null word embeddings: %d' % np.sum(np.sum(g_word_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4936, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=[]\n",
    "for ses_mod in data2:\n",
    "    Y.append(ses_mod['emotion'])\n",
    "    \n",
    "Y = label_binarize(Y,emotions_used)\n",
    "\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4936, 100, 34)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read speech feature data\n",
    "x_train_speech = np.load('voiced_feat_file_001_001.npy')\n",
    "x_train_speech.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE Keras API model\n",
    "text_input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "model_text = Embedding(nb_words,\n",
    "                      EMBEDDING_DIM,\n",
    "                      weights = [g_word_embedding_matrix],\n",
    "                      input_length = MAX_SEQUENCE_LENGTH)(text_input)\n",
    "conv1 = Convolution1D(256, 3, padding='same', activation='relu')(model_text)\n",
    "conv2 = Convolution1D(128, 3, padding='same', activation='relu')(conv1)\n",
    "conv3 = Convolution1D(64, 3, padding='same', activation='relu')(conv2)\n",
    "conv4 = Convolution1D(128, 3, padding='same', activation='relu')(conv3)\n",
    "flat = Flatten()(conv4)\n",
    "out_text = Dense(256)(flat)\n",
    "\n",
    "speech_input = Input(shape=(100,34))\n",
    "model_speech1 = Flatten()(speech_input)\n",
    "model_speech2 = Dense(1024, activation='relu')(model_speech1)\n",
    "model_speech3 = Dense(512, activation='relu')(model_speech2)\n",
    "out_speech = Dense(256, activation='relu')(model_speech3)\n",
    "\n",
    "model_combined1 = concatenate([out_text, out_speech])\n",
    "model_combined2 = Dense(256, activation='relu')(model_combined1)\n",
    "model_combined3 = Dense(4, activation='softmax')(model_combined2)\n",
    "\n",
    "# model compile\n",
    "model_combined = Model([text_input, speech_input], model_combined3)\n",
    "model_combined.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3948 samples, validate on 988 samples\n",
      "Epoch 1/25\n",
      "3948/3948 [==============================] - 40s 10ms/step - loss: 1.3214 - acc: 0.4179 - val_loss: 1.2025 - val_acc: 0.4686\n",
      "Epoch 2/25\n",
      "3948/3948 [==============================] - 38s 10ms/step - loss: 0.9046 - acc: 0.6142 - val_loss: 0.9247 - val_acc: 0.6366\n",
      "Epoch 3/25\n",
      "3948/3948 [==============================] - 38s 10ms/step - loss: 0.6569 - acc: 0.7391 - val_loss: 0.9836 - val_acc: 0.6417\n",
      "Epoch 4/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.4915 - acc: 0.8083 - val_loss: 0.9205 - val_acc: 0.6883\n",
      "Epoch 5/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.3831 - acc: 0.8551 - val_loss: 1.1980 - val_acc: 0.6680\n",
      "Epoch 6/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.3067 - acc: 0.8830 - val_loss: 1.3191 - val_acc: 0.6407\n",
      "Epoch 7/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.2640 - acc: 0.8969 - val_loss: 1.4754 - val_acc: 0.6832\n",
      "Epoch 8/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.2406 - acc: 0.9113 - val_loss: 1.4394 - val_acc: 0.6538\n",
      "Epoch 9/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.1841 - acc: 0.9309 - val_loss: 1.5738 - val_acc: 0.6498\n",
      "Epoch 10/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.2048 - acc: 0.9250 - val_loss: 1.7102 - val_acc: 0.6549\n",
      "Epoch 11/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.1504 - acc: 0.9433 - val_loss: 1.7372 - val_acc: 0.6538\n",
      "Epoch 12/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.1696 - acc: 0.9357 - val_loss: 1.8805 - val_acc: 0.6660\n",
      "Epoch 13/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.1470 - acc: 0.9438 - val_loss: 2.0439 - val_acc: 0.6549\n",
      "Epoch 14/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.1250 - acc: 0.9509 - val_loss: 2.0600 - val_acc: 0.6437\n",
      "Epoch 15/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.1254 - acc: 0.9504 - val_loss: 2.2164 - val_acc: 0.6478\n",
      "Epoch 16/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.1132 - acc: 0.9554 - val_loss: 2.1412 - val_acc: 0.6650\n",
      "Epoch 17/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.1549 - acc: 0.9445 - val_loss: 2.6581 - val_acc: 0.5962\n",
      "Epoch 18/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.1696 - acc: 0.9417 - val_loss: 2.1693 - val_acc: 0.6387\n",
      "Epoch 19/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.0989 - acc: 0.9645 - val_loss: 2.2604 - val_acc: 0.6346\n",
      "Epoch 20/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.0950 - acc: 0.9600 - val_loss: 2.2771 - val_acc: 0.6488\n",
      "Epoch 21/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.0777 - acc: 0.9704 - val_loss: 2.4334 - val_acc: 0.6650\n",
      "Epoch 22/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.0682 - acc: 0.9764 - val_loss: 2.6796 - val_acc: 0.6306\n",
      "Epoch 23/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.1262 - acc: 0.9554 - val_loss: 2.6795 - val_acc: 0.6093\n",
      "Epoch 24/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.2057 - acc: 0.9326 - val_loss: 2.3210 - val_acc: 0.6265\n",
      "Epoch 25/25\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.1307 - acc: 0.9569 - val_loss: 2.1724 - val_acc: 0.6518\n"
     ]
    }
   ],
   "source": [
    "hist = model_combined.fit([x_train_text, x_train_speech], Y, batch_size=16, epochs=25, verbose=1, \n",
    "                          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6407287450840599 0.6882591090704265\n"
     ]
    }
   ],
   "source": [
    "acc1 = hist.history['val_acc']\n",
    "print(np.mean(acc1), max(acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_47 (InputLayer)           (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_32 (Embedding)        (None, 500, 128)     350336      input_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_48 (InputLayer)           (None, 100, 34)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_50 (Flatten)            (None, 64000)        0           embedding_32[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_51 (Flatten)            (None, 3400)         0           input_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_145 (Dense)               (None, 1024)         65537024    flatten_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_148 (Dense)               (None, 1024)         3482624     flatten_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 1024)         0           dense_145[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 1024)         0           dense_148[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_146 (Dense)               (None, 512)          524800      dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_149 (Dense)               (None, 512)          524800      dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 512)          0           dense_146[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 512)          0           dense_149[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_147 (Dense)               (None, 256)          131328      dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_150 (Dense)               (None, 256)          131328      dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 512)          0           dense_147[0][0]                  \n",
      "                                                                 dense_150[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_151 (Dense)               (None, 256)          131328      concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_152 (Dense)               (None, 4)            1028        dense_151[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 70,814,596\n",
      "Trainable params: 70,814,596\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model2 Built\n"
     ]
    }
   ],
   "source": [
    "# text model\n",
    "text_input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "model_text1 = (Embedding(2737, 128, input_length=500))(text_input)\n",
    "model_text2 = Flatten()(model_text1)\n",
    "model_text3 = Dense(1024, activation='relu')(model_text2)\n",
    "model_text4 = Dropout(0.2)(model_text3)\n",
    "model_text5 = Dense(512, activation='relu')(model_text4)\n",
    "model_text6 = Dropout(0.2)(model_text5)\n",
    "model_text = Dense(256)(model_text6)\n",
    "\n",
    "# speech model\n",
    "speech_input = Input(shape=(100,34))\n",
    "model_speech1 = Flatten()(speech_input)\n",
    "model_speech2 = Dense(1024, activation='relu')(model_speech1)\n",
    "model_speech3 = Dropout(0.2)(model_speech2)\n",
    "model_speech4 = Dense(512, activation='relu')(model_speech3)\n",
    "model_speech5 = Dropout(0.2)(model_speech4)\n",
    "model_speech = Dense(256)(model_speech5)\n",
    "\n",
    "# combined model\n",
    "model_combined1 = concatenate([model_text, model_speech])\n",
    "model_combined2 = Dense(256, activation='relu')(model_combined1)\n",
    "model_combined3 = Dense(4, activation='softmax')(model_combined2)\n",
    "\n",
    "model_combined =  Model([text_input, speech_input], model_combined3)\n",
    "model_combined.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "## compille it here according to instructions\n",
    "\n",
    "#model.compile()\n",
    "#model_speech.summary()\n",
    "#model_text.summary()\n",
    "model_combined.summary()\n",
    "\n",
    "print(\"Model2 Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3948 samples, validate on 988 samples\n",
      "Epoch 1/30\n",
      "3948/3948 [==============================] - 41s 10ms/step - loss: 2.1079 - acc: 0.3549 - val_loss: 1.2401 - val_acc: 0.4130\n",
      "Epoch 2/30\n",
      "3948/3948 [==============================] - 37s 9ms/step - loss: 1.1848 - acc: 0.4466 - val_loss: 1.1269 - val_acc: 0.4899\n",
      "Epoch 3/30\n",
      "3948/3948 [==============================] - 38s 10ms/step - loss: 0.8826 - acc: 0.6261 - val_loss: 0.9908 - val_acc: 0.5496\n",
      "Epoch 4/30\n",
      "3948/3948 [==============================] - 38s 10ms/step - loss: 0.5351 - acc: 0.7880 - val_loss: 1.1208 - val_acc: 0.6204\n",
      "Epoch 5/30\n",
      "3948/3948 [==============================] - 38s 10ms/step - loss: 0.3433 - acc: 0.8693 - val_loss: 1.2113 - val_acc: 0.6154\n",
      "Epoch 6/30\n",
      "3948/3948 [==============================] - 38s 10ms/step - loss: 0.2635 - acc: 0.9017 - val_loss: 1.2088 - val_acc: 0.6012\n",
      "Epoch 7/30\n",
      "3948/3948 [==============================] - 38s 10ms/step - loss: 0.2409 - acc: 0.9113 - val_loss: 1.2674 - val_acc: 0.6326\n",
      "Epoch 8/30\n",
      "3948/3948 [==============================] - 38s 10ms/step - loss: 0.1985 - acc: 0.9293 - val_loss: 1.4462 - val_acc: 0.6245\n",
      "Epoch 9/30\n",
      "3948/3948 [==============================] - 38s 10ms/step - loss: 0.1775 - acc: 0.9334 - val_loss: 1.5036 - val_acc: 0.6245\n",
      "Epoch 10/30\n",
      "3948/3948 [==============================] - 38s 10ms/step - loss: 0.1645 - acc: 0.9364 - val_loss: 1.4597 - val_acc: 0.6296\n",
      "Epoch 11/30\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.1574 - acc: 0.9435 - val_loss: 1.5353 - val_acc: 0.6387\n",
      "Epoch 12/30\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.1332 - acc: 0.9460 - val_loss: 1.9765 - val_acc: 0.6093\n",
      "Epoch 13/30\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.1406 - acc: 0.9483 - val_loss: 1.9784 - val_acc: 0.5982\n",
      "Epoch 14/30\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.1253 - acc: 0.9544 - val_loss: 1.8960 - val_acc: 0.6022\n",
      "Epoch 15/30\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.3381 - acc: 0.8984 - val_loss: 1.5169 - val_acc: 0.5709\n",
      "Epoch 16/30\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.2425 - acc: 0.9113 - val_loss: 2.1389 - val_acc: 0.5962\n",
      "Epoch 17/30\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.2303 - acc: 0.9159 - val_loss: 1.4799 - val_acc: 0.5526\n",
      "Epoch 18/30\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.3198 - acc: 0.8769 - val_loss: 1.9533 - val_acc: 0.6063\n",
      "Epoch 19/30\n",
      "3948/3948 [==============================] - 39s 10ms/step - loss: 0.1193 - acc: 0.9547 - val_loss: 2.0311 - val_acc: 0.6164\n",
      "Epoch 20/30\n",
      "3948/3948 [==============================] - 41s 10ms/step - loss: 0.1038 - acc: 0.9577 - val_loss: 2.2419 - val_acc: 0.6093\n",
      "Epoch 21/30\n",
      "3948/3948 [==============================] - 41s 10ms/step - loss: 0.0965 - acc: 0.9618 - val_loss: 2.3499 - val_acc: 0.6184\n",
      "Epoch 22/30\n",
      "3948/3948 [==============================] - 42s 11ms/step - loss: 0.0897 - acc: 0.9668 - val_loss: 2.4188 - val_acc: 0.6204\n",
      "Epoch 23/30\n",
      "3948/3948 [==============================] - 42s 11ms/step - loss: 0.0854 - acc: 0.9653 - val_loss: 2.6355 - val_acc: 0.6083\n",
      "Epoch 24/30\n",
      "3948/3948 [==============================] - 41s 10ms/step - loss: 0.0796 - acc: 0.9696 - val_loss: 2.3864 - val_acc: 0.6134\n",
      "Epoch 25/30\n",
      "3948/3948 [==============================] - 40s 10ms/step - loss: 0.0717 - acc: 0.9721 - val_loss: 2.9154 - val_acc: 0.5931\n",
      "Epoch 26/30\n",
      "3948/3948 [==============================] - 41s 10ms/step - loss: 0.0767 - acc: 0.9673 - val_loss: 2.8645 - val_acc: 0.5972\n",
      "Epoch 27/30\n",
      "3948/3948 [==============================] - 41s 10ms/step - loss: 0.0689 - acc: 0.9711 - val_loss: 3.2239 - val_acc: 0.6012\n",
      "Epoch 28/30\n",
      "3948/3948 [==============================] - 42s 11ms/step - loss: 0.0724 - acc: 0.9714 - val_loss: 3.1830 - val_acc: 0.5881\n",
      "Epoch 29/30\n",
      "3948/3948 [==============================] - 43s 11ms/step - loss: 0.0837 - acc: 0.9663 - val_loss: 2.4522 - val_acc: 0.6073\n",
      "Epoch 30/30\n",
      "3948/3948 [==============================] - 43s 11ms/step - loss: 0.0924 - acc: 0.9610 - val_loss: 2.7997 - val_acc: 0.6123\n"
     ]
    }
   ],
   "source": [
    "hist = model_combined.fit([x_train_text,x_train_speech], Y, \n",
    "                 batch_size=32, epochs=30, verbose=1, \n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.638663966887393\n"
     ]
    }
   ],
   "source": [
    "acc2 = max(hist.history['val_acc'])\n",
    "print(acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_59 (InputLayer)           (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_60 (InputLayer)           (None, 100, 34)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_38 (Embedding)        (None, 500, 128)     350336      input_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_57 (Flatten)            (None, 3400)         0           input_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  (None, 500, 256)     394240      embedding_38[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_179 (Dense)               (None, 1024)         3482624     flatten_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  (None, 256)          525312      lstm_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 1024)         0           dense_179[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_178 (Dense)               (None, 256)          65792       lstm_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_180 (Dense)               (None, 256)          262400      dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 512)          0           dense_178[0][0]                  \n",
      "                                                                 dense_180[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_181 (Dense)               (None, 256)          131328      concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_182 (Dense)               (None, 4)            1028        dense_181[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,213,060\n",
      "Trainable params: 5,213,060\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model3 Built\n"
     ]
    }
   ],
   "source": [
    "text_input = Input(shape=(MAX_SEQUENCE_LENGTH, ))\n",
    "model_text1 = Embedding(2737, 128, input_length=500)(text_input)\n",
    "\n",
    "model_text2 = LSTM(256, return_sequences=True)(model_text1)\n",
    "model_text3 = LSTM(256, return_sequences=False)(model_text2)\n",
    "model_text = Dense(256)(model_text3)\n",
    "\n",
    "speech_input = Input(shape=(100, 34))\n",
    "model_speech1 = Flatten()(speech_input)\n",
    "model_speech2 = Dense(1024, activation='relu')(model_speech1)\n",
    "model_speech3 = Dropout(0.2)(model_speech2)\n",
    "model_speech = Dense(256)(model_speech3)\n",
    "\n",
    "# combination of text and speech\n",
    "model_combined1 = concatenate([model_text, model_speech])\n",
    "model_combined2 = Dense(256, activation='relu')(model_combined1)\n",
    "model_combined3 = Dense(4, activation='softmax')(model_combined2)\n",
    "\n",
    "model_combined = Model([text_input, speech_input], model_combined3)\n",
    "model_combined.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "#model.compile()\n",
    "model_combined.summary()\n",
    "\n",
    "print(\"Model3 Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3455 samples, validate on 1481 samples\n",
      "Epoch 1/25\n",
      "3455/3455 [==============================] - 145s 42ms/step - loss: 0.0126 - acc: 0.9977 - val_loss: 2.5782 - val_acc: 0.7427\n",
      "Epoch 2/25\n",
      "3455/3455 [==============================] - 127s 37ms/step - loss: 0.0097 - acc: 0.9986 - val_loss: 2.5879 - val_acc: 0.7448\n",
      "Epoch 3/25\n",
      "3455/3455 [==============================] - 120s 35ms/step - loss: 0.0103 - acc: 0.9983 - val_loss: 2.5959 - val_acc: 0.7468\n",
      "Epoch 4/25\n",
      "3455/3455 [==============================] - 146s 42ms/step - loss: 0.0134 - acc: 0.9977 - val_loss: 2.5780 - val_acc: 0.7475\n",
      "Epoch 5/25\n",
      "3455/3455 [==============================] - 145s 42ms/step - loss: 0.0078 - acc: 0.9988 - val_loss: 2.5949 - val_acc: 0.7481\n",
      "Epoch 6/25\n",
      "3455/3455 [==============================] - 145s 42ms/step - loss: 0.0092 - acc: 0.9986 - val_loss: 2.6298 - val_acc: 0.7454\n",
      "Epoch 7/25\n",
      "3455/3455 [==============================] - 145s 42ms/step - loss: 0.0100 - acc: 0.9980 - val_loss: 2.6489 - val_acc: 0.7448\n",
      "Epoch 8/25\n",
      "3455/3455 [==============================] - 133s 39ms/step - loss: 0.0112 - acc: 0.9980 - val_loss: 2.6505 - val_acc: 0.7407\n",
      "Epoch 9/25\n",
      "3455/3455 [==============================] - 114s 33ms/step - loss: 0.0085 - acc: 0.9988 - val_loss: 2.6466 - val_acc: 0.7434\n",
      "Epoch 10/25\n",
      "3455/3455 [==============================] - 147s 43ms/step - loss: 0.0114 - acc: 0.9977 - val_loss: 2.6639 - val_acc: 0.7441\n",
      "Epoch 11/25\n",
      "3455/3455 [==============================] - 146s 42ms/step - loss: 0.0097 - acc: 0.9986 - val_loss: 2.6452 - val_acc: 0.7454\n",
      "Epoch 12/25\n",
      "3455/3455 [==============================] - 145s 42ms/step - loss: 0.0083 - acc: 0.9988 - val_loss: 2.6619 - val_acc: 0.7427\n",
      "Epoch 13/25\n",
      "3455/3455 [==============================] - 145s 42ms/step - loss: 0.0086 - acc: 0.9983 - val_loss: 2.6679 - val_acc: 0.7448\n",
      "Epoch 14/25\n",
      "3455/3455 [==============================] - 136s 39ms/step - loss: 0.0102 - acc: 0.9974 - val_loss: 2.7050 - val_acc: 0.7441\n",
      "Epoch 15/25\n",
      "3455/3455 [==============================] - 111s 32ms/step - loss: 0.0086 - acc: 0.9986 - val_loss: 2.7286 - val_acc: 0.7448\n",
      "Epoch 16/25\n",
      "3455/3455 [==============================] - 146s 42ms/step - loss: 0.0089 - acc: 0.9986 - val_loss: 2.7522 - val_acc: 0.7454\n",
      "Epoch 17/25\n",
      "3455/3455 [==============================] - 146s 42ms/step - loss: 0.0073 - acc: 0.9983 - val_loss: 2.7652 - val_acc: 0.7488\n",
      "Epoch 18/25\n",
      "3455/3455 [==============================] - 145s 42ms/step - loss: 0.0079 - acc: 0.9988 - val_loss: 2.7042 - val_acc: 0.7421\n",
      "Epoch 19/25\n",
      "3455/3455 [==============================] - 145s 42ms/step - loss: 0.0098 - acc: 0.9971 - val_loss: 2.7729 - val_acc: 0.7488\n",
      "Epoch 20/25\n",
      "3455/3455 [==============================] - 143s 41ms/step - loss: 0.0075 - acc: 0.9991 - val_loss: 2.8101 - val_acc: 0.7502\n",
      "Epoch 21/25\n",
      "3455/3455 [==============================] - 108s 31ms/step - loss: 0.0089 - acc: 0.9980 - val_loss: 2.7644 - val_acc: 0.7549\n",
      "Epoch 22/25\n",
      "3455/3455 [==============================] - 147s 42ms/step - loss: 0.0100 - acc: 0.9977 - val_loss: 2.8024 - val_acc: 0.7468\n",
      "Epoch 23/25\n",
      "3455/3455 [==============================] - 145s 42ms/step - loss: 0.0074 - acc: 0.9988 - val_loss: 2.7951 - val_acc: 0.7481\n",
      "Epoch 24/25\n",
      "3455/3455 [==============================] - 145s 42ms/step - loss: 0.0106 - acc: 0.9971 - val_loss: 2.7647 - val_acc: 0.7481\n",
      "Epoch 25/25\n",
      "3455/3455 [==============================] - 145s 42ms/step - loss: 0.0123 - acc: 0.9962 - val_loss: 2.8422 - val_acc: 0.7448\n"
     ]
    }
   ],
   "source": [
    "hist = model_combined.fit([x_train_text, x_train_speech], Y, batch_size=64, epochs=25, verbose=1, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7459284267386902 0.7548953409858203\n"
     ]
    }
   ],
   "source": [
    "acc3=hist.history['val_acc']\n",
    "print(np.mean(acc3), max(acc3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
