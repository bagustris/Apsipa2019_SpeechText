{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import wave\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import LSTM, Input, Flatten, Merge, Embedding, Convolution1D, Dropout, Bidirectional\n",
    "from attention_helper import *\n",
    "\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "from features import *\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "emotions_used = np.array(['ang', 'exc', 'neu', 'sad'])\n",
    "data_path = '/media/bagus/data01/dataset/IEMOCAP_full_release/'\n",
    "sessions = ['Session1', 'Session2', 'Session3', 'Session4', 'Session5']\n",
    "framerate = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/media/bagus/data01/dataset/IEMOCAP_full_release/data_collected.pickle', 'rb') as handle:\n",
    "    data2 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "\n",
    "for ses_mod in data2:\n",
    "    text.append(ses_mod['transcription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "token_tr_X = tokenizer.texts_to_sequences(text)\n",
    "x_train_text = []\n",
    "\n",
    "x_train_text = sequence.pad_sequences(token_tr_X, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2736 unique tokens\n",
      "../../data/glove.840B.300d.txt\n",
      "G Word embeddings: 2196018\n",
      "G Null word embeddings: 100\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "file_loc = '../../data/glove.840B.300d.txt'\n",
    "\n",
    "print (file_loc)\n",
    "\n",
    "gembeddings_index = {}\n",
    "with codecs.open(file_loc, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        gembedding = np.asarray(values[1:], dtype='float32')\n",
    "        gembeddings_index[word] = gembedding\n",
    "#\n",
    "f.close()\n",
    "print('G Word embeddings:', len(gembeddings_index))\n",
    "\n",
    "nb_words = len(word_index) +1\n",
    "g_word_embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    gembedding_vector = gembeddings_index.get(word)\n",
    "    if gembedding_vector is not None:\n",
    "        g_word_embedding_matrix[i] = gembedding_vector\n",
    "        \n",
    "print('G Null word embeddings: %d' % np.sum(np.sum(g_word_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4936, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=[]\n",
    "for ses_mod in data2:\n",
    "    Y.append(ses_mod['emotion'])\n",
    "    \n",
    "Y = label_binarize(Y,emotions_used)\n",
    "\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/bagus/data01/github/IEMOCAP-Emotion-Detection/er/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1205: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /media/bagus/data01/github/IEMOCAP-Emotion-Detection/er/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2755: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /media/bagus/data01/github/IEMOCAP-Emotion-Detection/er/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 300)          821100    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 256)          230656    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 256)          0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 500, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 128)          98432     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 500, 64)           24640     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 500, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               4096256   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 1028      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 5,278,288\n",
      "Trainable params: 5,278,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model1 Built\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#model.add(Embedding(2737, 128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Embedding(nb_words,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights = [g_word_embedding_matrix],\n",
    "                    input_length = MAX_SEQUENCE_LENGTH,\n",
    "                    trainable = True))\n",
    "model.add(Convolution1D(256, 3, padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(128, 3, padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(64, 3, padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(32, 3, padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam' ,metrics=['acc'])\n",
    "\n",
    "#model.compile()\n",
    "model.summary()\n",
    "\n",
    "print(\"Model1 Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/bagus/data01/github/IEMOCAP-Emotion-Detection/er/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3948 samples, validate on 988 samples\n",
      "Epoch 1/125\n",
      "3948/3948 [==============================] - 17s - loss: 1.3682 - acc: 0.3437 - val_loss: 1.3326 - val_acc: 0.3897\n",
      "Epoch 2/125\n",
      "3948/3948 [==============================] - 16s - loss: 1.3042 - acc: 0.3713 - val_loss: 1.2245 - val_acc: 0.4231\n",
      "Epoch 3/125\n",
      "3948/3948 [==============================] - 16s - loss: 1.1635 - acc: 0.4607 - val_loss: 1.1442 - val_acc: 0.4828\n",
      "Epoch 4/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.9722 - acc: 0.5798 - val_loss: 1.0299 - val_acc: 0.5840\n",
      "Epoch 5/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.7775 - acc: 0.6811 - val_loss: 0.9788 - val_acc: 0.6316\n",
      "Epoch 6/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.6349 - acc: 0.7543 - val_loss: 1.0378 - val_acc: 0.6174\n",
      "Epoch 7/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.5266 - acc: 0.7936 - val_loss: 1.1187 - val_acc: 0.6053\n",
      "Epoch 8/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.4264 - acc: 0.8343 - val_loss: 1.2359 - val_acc: 0.6275\n",
      "Epoch 9/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.4003 - acc: 0.8485 - val_loss: 1.3857 - val_acc: 0.6285\n",
      "Epoch 10/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.3295 - acc: 0.8739 - val_loss: 1.3257 - val_acc: 0.6306\n",
      "Epoch 11/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.2831 - acc: 0.9002 - val_loss: 1.5129 - val_acc: 0.6123\n",
      "Epoch 12/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.2883 - acc: 0.8916 - val_loss: 1.4747 - val_acc: 0.6174\n",
      "Epoch 13/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.2913 - acc: 0.8875 - val_loss: 1.4964 - val_acc: 0.6326\n",
      "Epoch 14/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.2700 - acc: 0.8999 - val_loss: 1.4060 - val_acc: 0.6285\n",
      "Epoch 15/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.2341 - acc: 0.9096 - val_loss: 1.6517 - val_acc: 0.6488\n",
      "Epoch 16/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.2292 - acc: 0.9119 - val_loss: 1.7249 - val_acc: 0.6204\n",
      "Epoch 17/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.2053 - acc: 0.9182 - val_loss: 1.8409 - val_acc: 0.6144\n",
      "Epoch 18/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1993 - acc: 0.9195 - val_loss: 1.6926 - val_acc: 0.6518\n",
      "Epoch 19/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1952 - acc: 0.9271 - val_loss: 1.7509 - val_acc: 0.6306\n",
      "Epoch 20/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1836 - acc: 0.9260 - val_loss: 1.9128 - val_acc: 0.6417\n",
      "Epoch 21/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1754 - acc: 0.9248 - val_loss: 2.0125 - val_acc: 0.6164\n",
      "Epoch 22/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1957 - acc: 0.9202 - val_loss: 1.6587 - val_acc: 0.6215\n",
      "Epoch 23/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1769 - acc: 0.9245 - val_loss: 1.9835 - val_acc: 0.6437\n",
      "Epoch 24/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1748 - acc: 0.9286 - val_loss: 2.2498 - val_acc: 0.6356\n",
      "Epoch 25/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1804 - acc: 0.9301 - val_loss: 1.9624 - val_acc: 0.6427\n",
      "Epoch 26/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1681 - acc: 0.9324 - val_loss: 2.0558 - val_acc: 0.6356\n",
      "Epoch 27/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1780 - acc: 0.9293 - val_loss: 1.8907 - val_acc: 0.6316\n",
      "Epoch 28/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1631 - acc: 0.9334 - val_loss: 2.0066 - val_acc: 0.6215\n",
      "Epoch 29/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1558 - acc: 0.9384 - val_loss: 2.0947 - val_acc: 0.6437\n",
      "Epoch 30/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1517 - acc: 0.9357 - val_loss: 2.2328 - val_acc: 0.6194\n",
      "Epoch 31/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1640 - acc: 0.9311 - val_loss: 2.0066 - val_acc: 0.6285\n",
      "Epoch 32/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1551 - acc: 0.9349 - val_loss: 2.1031 - val_acc: 0.6204\n",
      "Epoch 33/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1402 - acc: 0.9412 - val_loss: 2.2632 - val_acc: 0.6174\n",
      "Epoch 34/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1470 - acc: 0.9372 - val_loss: 1.9904 - val_acc: 0.6316\n",
      "Epoch 35/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1467 - acc: 0.9387 - val_loss: 2.2690 - val_acc: 0.6326\n",
      "Epoch 36/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1739 - acc: 0.9293 - val_loss: 2.1070 - val_acc: 0.6417\n",
      "Epoch 37/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1486 - acc: 0.9377 - val_loss: 2.3123 - val_acc: 0.6488\n",
      "Epoch 38/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1388 - acc: 0.9412 - val_loss: 2.4525 - val_acc: 0.6002\n",
      "Epoch 39/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1338 - acc: 0.9352 - val_loss: 2.3766 - val_acc: 0.6285\n",
      "Epoch 40/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1434 - acc: 0.9387 - val_loss: 2.2799 - val_acc: 0.6407\n",
      "Epoch 41/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1556 - acc: 0.9324 - val_loss: 2.1710 - val_acc: 0.6225\n",
      "Epoch 42/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1394 - acc: 0.9407 - val_loss: 2.4680 - val_acc: 0.6285\n",
      "Epoch 43/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1629 - acc: 0.9319 - val_loss: 2.2747 - val_acc: 0.6285\n",
      "Epoch 44/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1855 - acc: 0.9233 - val_loss: 2.2992 - val_acc: 0.6043\n",
      "Epoch 45/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1416 - acc: 0.9425 - val_loss: 2.5159 - val_acc: 0.6427\n",
      "Epoch 46/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1460 - acc: 0.9364 - val_loss: 2.4396 - val_acc: 0.6215\n",
      "Epoch 47/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1421 - acc: 0.9354 - val_loss: 2.3807 - val_acc: 0.6204\n",
      "Epoch 48/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1528 - acc: 0.9329 - val_loss: 2.3213 - val_acc: 0.6154\n",
      "Epoch 49/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1489 - acc: 0.9309 - val_loss: 2.3692 - val_acc: 0.6255\n",
      "Epoch 50/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1400 - acc: 0.9377 - val_loss: 2.4908 - val_acc: 0.6255\n",
      "Epoch 51/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1334 - acc: 0.9417 - val_loss: 2.4529 - val_acc: 0.6356\n",
      "Epoch 52/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1352 - acc: 0.9402 - val_loss: 2.5120 - val_acc: 0.6387\n",
      "Epoch 53/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1363 - acc: 0.9362 - val_loss: 2.5403 - val_acc: 0.6184\n",
      "Epoch 54/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1341 - acc: 0.9392 - val_loss: 2.4698 - val_acc: 0.6326\n",
      "Epoch 55/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1212 - acc: 0.9458 - val_loss: 2.8710 - val_acc: 0.6245\n",
      "Epoch 56/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1248 - acc: 0.9443 - val_loss: 2.6457 - val_acc: 0.6316\n",
      "Epoch 57/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1191 - acc: 0.9448 - val_loss: 2.8046 - val_acc: 0.6113\n",
      "Epoch 58/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1272 - acc: 0.9415 - val_loss: 2.8472 - val_acc: 0.5931\n",
      "Epoch 59/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1188 - acc: 0.9438 - val_loss: 2.9288 - val_acc: 0.6306\n",
      "Epoch 60/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1263 - acc: 0.9430 - val_loss: 2.7213 - val_acc: 0.6093\n",
      "Epoch 61/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1355 - acc: 0.9377 - val_loss: 2.7395 - val_acc: 0.6144\n",
      "Epoch 62/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1326 - acc: 0.9387 - val_loss: 2.6827 - val_acc: 0.6164\n",
      "Epoch 63/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1366 - acc: 0.9364 - val_loss: 2.7998 - val_acc: 0.6316\n",
      "Epoch 64/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1376 - acc: 0.9339 - val_loss: 2.6718 - val_acc: 0.6447\n",
      "Epoch 65/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1221 - acc: 0.9458 - val_loss: 2.9235 - val_acc: 0.6296\n",
      "Epoch 66/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1313 - acc: 0.9364 - val_loss: 2.7854 - val_acc: 0.6204\n",
      "Epoch 67/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1366 - acc: 0.9352 - val_loss: 2.7659 - val_acc: 0.6417\n",
      "Epoch 68/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1327 - acc: 0.9395 - val_loss: 2.7683 - val_acc: 0.6316\n",
      "Epoch 69/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1332 - acc: 0.9405 - val_loss: 2.6932 - val_acc: 0.6316\n",
      "Epoch 70/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1237 - acc: 0.9425 - val_loss: 2.8535 - val_acc: 0.6296\n",
      "Epoch 71/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1304 - acc: 0.9445 - val_loss: 2.9439 - val_acc: 0.6245\n",
      "Epoch 72/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1334 - acc: 0.9412 - val_loss: 2.8717 - val_acc: 0.6296\n",
      "Epoch 73/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1270 - acc: 0.9402 - val_loss: 2.8648 - val_acc: 0.6245\n",
      "Epoch 74/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1201 - acc: 0.9445 - val_loss: 2.7740 - val_acc: 0.6447\n",
      "Epoch 75/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1205 - acc: 0.9453 - val_loss: 2.9453 - val_acc: 0.6164\n",
      "Epoch 76/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1263 - acc: 0.9425 - val_loss: 2.7813 - val_acc: 0.6306\n",
      "Epoch 77/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1277 - acc: 0.9433 - val_loss: 2.8122 - val_acc: 0.6407\n",
      "Epoch 78/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1421 - acc: 0.9390 - val_loss: 2.5471 - val_acc: 0.6326\n",
      "Epoch 79/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1213 - acc: 0.9425 - val_loss: 2.8277 - val_acc: 0.6123\n",
      "Epoch 80/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1286 - acc: 0.9453 - val_loss: 2.9980 - val_acc: 0.6123\n",
      "Epoch 81/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1248 - acc: 0.9438 - val_loss: 2.8872 - val_acc: 0.6306\n",
      "Epoch 82/125\n",
      "3948/3948 [==============================] - 15s - loss: 0.1419 - acc: 0.9352 - val_loss: 2.6464 - val_acc: 0.6073\n",
      "Epoch 83/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1233 - acc: 0.9407 - val_loss: 2.8365 - val_acc: 0.6032\n",
      "Epoch 84/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1232 - acc: 0.9478 - val_loss: 2.8508 - val_acc: 0.6184\n",
      "Epoch 85/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1204 - acc: 0.9415 - val_loss: 3.0190 - val_acc: 0.6265\n",
      "Epoch 86/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1333 - acc: 0.9359 - val_loss: 2.7514 - val_acc: 0.6275\n",
      "Epoch 87/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1240 - acc: 0.9433 - val_loss: 2.7637 - val_acc: 0.6144\n",
      "Epoch 88/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1209 - acc: 0.9438 - val_loss: 2.7384 - val_acc: 0.6275\n",
      "Epoch 89/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1170 - acc: 0.9433 - val_loss: 3.0234 - val_acc: 0.6215\n",
      "Epoch 90/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1178 - acc: 0.9445 - val_loss: 2.9837 - val_acc: 0.6174\n",
      "Epoch 91/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1120 - acc: 0.9481 - val_loss: 2.9572 - val_acc: 0.6275\n",
      "Epoch 92/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1092 - acc: 0.9453 - val_loss: 3.0172 - val_acc: 0.6326\n",
      "Epoch 93/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1149 - acc: 0.9468 - val_loss: 2.9764 - val_acc: 0.6407\n",
      "Epoch 94/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1241 - acc: 0.9392 - val_loss: 3.0331 - val_acc: 0.6225\n",
      "Epoch 95/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1472 - acc: 0.9392 - val_loss: 2.8091 - val_acc: 0.6255\n",
      "Epoch 96/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1375 - acc: 0.9390 - val_loss: 2.8911 - val_acc: 0.6275\n",
      "Epoch 97/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1383 - acc: 0.9395 - val_loss: 2.6827 - val_acc: 0.6366\n",
      "Epoch 98/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1364 - acc: 0.9400 - val_loss: 2.9826 - val_acc: 0.6134\n",
      "Epoch 99/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1224 - acc: 0.9422 - val_loss: 2.7409 - val_acc: 0.6447\n",
      "Epoch 100/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1187 - acc: 0.9445 - val_loss: 2.8828 - val_acc: 0.6427\n",
      "Epoch 101/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1161 - acc: 0.9440 - val_loss: 3.1049 - val_acc: 0.6194\n",
      "Epoch 102/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1119 - acc: 0.9450 - val_loss: 3.0276 - val_acc: 0.6457\n",
      "Epoch 103/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1185 - acc: 0.9471 - val_loss: 2.9341 - val_acc: 0.6569\n",
      "Epoch 104/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1415 - acc: 0.9397 - val_loss: 2.8576 - val_acc: 0.6407\n",
      "Epoch 105/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1289 - acc: 0.9395 - val_loss: 2.7870 - val_acc: 0.6306\n",
      "Epoch 106/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1253 - acc: 0.9425 - val_loss: 2.7854 - val_acc: 0.6457\n",
      "Epoch 107/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1191 - acc: 0.9471 - val_loss: 2.9800 - val_acc: 0.6306\n",
      "Epoch 108/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1171 - acc: 0.9468 - val_loss: 2.9688 - val_acc: 0.6377\n",
      "Epoch 109/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1122 - acc: 0.9445 - val_loss: 3.2456 - val_acc: 0.6387\n",
      "Epoch 110/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1265 - acc: 0.9397 - val_loss: 2.9909 - val_acc: 0.6204\n",
      "Epoch 111/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1216 - acc: 0.9402 - val_loss: 3.0003 - val_acc: 0.6306\n",
      "Epoch 112/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1212 - acc: 0.9412 - val_loss: 3.1360 - val_acc: 0.6356\n",
      "Epoch 113/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1287 - acc: 0.9422 - val_loss: 3.0419 - val_acc: 0.6336\n",
      "Epoch 114/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1356 - acc: 0.9400 - val_loss: 2.9192 - val_acc: 0.6316\n",
      "Epoch 115/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1291 - acc: 0.9417 - val_loss: 2.9808 - val_acc: 0.6306\n",
      "Epoch 116/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1207 - acc: 0.9458 - val_loss: 2.9862 - val_acc: 0.6437\n",
      "Epoch 117/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1200 - acc: 0.9420 - val_loss: 3.1203 - val_acc: 0.6316\n",
      "Epoch 118/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1265 - acc: 0.9422 - val_loss: 2.9436 - val_acc: 0.6194\n",
      "Epoch 119/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1152 - acc: 0.9460 - val_loss: 2.9945 - val_acc: 0.6215\n",
      "Epoch 120/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1204 - acc: 0.9438 - val_loss: 3.0892 - val_acc: 0.6306\n",
      "Epoch 121/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1119 - acc: 0.9476 - val_loss: 3.1020 - val_acc: 0.6356\n",
      "Epoch 122/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1124 - acc: 0.9458 - val_loss: 3.2033 - val_acc: 0.6316\n",
      "Epoch 123/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1068 - acc: 0.9463 - val_loss: 3.2057 - val_acc: 0.6144\n",
      "Epoch 124/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1357 - acc: 0.9377 - val_loss: 3.1811 - val_acc: 0.6204\n",
      "Epoch 125/125\n",
      "3948/3948 [==============================] - 16s - loss: 0.1226 - acc: 0.9430 - val_loss: 3.0665 - val_acc: 0.6134\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_text, Y, \n",
    "                 batch_size=100, epochs=125, verbose=1, \n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65688259012786\n"
     ]
    }
   ],
   "source": [
    "acc1 = max(hist.history['val_acc'])\n",
    "print(acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 500, 300)          821100    \n",
      "_________________________________________________________________\n",
      "lstm_71 (LSTM)               (None, 500, 512)          1665024   \n",
      "_________________________________________________________________\n",
      "lstm_72 (LSTM)               (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 3,407,216\n",
      "Trainable params: 3,407,216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model2 Built\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(nb_words,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights = [g_word_embedding_matrix],\n",
    "                    input_length = MAX_SEQUENCE_LENGTH,\n",
    "                    trainable = True))\n",
    "\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(LSTM(256, return_sequences=False))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "#model.compile()\n",
    "model.summary()\n",
    "\n",
    "print(\"Model2 Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3948 samples, validate on 988 samples\n",
      "Epoch 1/125\n",
      "3948/3948 [==============================] - 241s - loss: 1.3100 - acc: 0.3954 - val_loss: 1.1864 - val_acc: 0.4626\n",
      "Epoch 2/125\n",
      "3948/3948 [==============================] - 238s - loss: 1.0652 - acc: 0.5484 - val_loss: 1.0054 - val_acc: 0.6032\n",
      "Epoch 3/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.8480 - acc: 0.6631 - val_loss: 1.0126 - val_acc: 0.5749\n",
      "Epoch 4/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.7060 - acc: 0.7239 - val_loss: 0.9911 - val_acc: 0.6225\n",
      "Epoch 5/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.5972 - acc: 0.7573 - val_loss: 0.9178 - val_acc: 0.6508\n",
      "Epoch 6/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.5017 - acc: 0.8060 - val_loss: 0.9980 - val_acc: 0.6387\n",
      "Epoch 7/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.4360 - acc: 0.8341 - val_loss: 1.1513 - val_acc: 0.6306\n",
      "Epoch 8/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.4280 - acc: 0.8338 - val_loss: 1.0927 - val_acc: 0.6387\n",
      "Epoch 9/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.3597 - acc: 0.8579 - val_loss: 1.2547 - val_acc: 0.6437\n",
      "Epoch 10/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.3365 - acc: 0.8663 - val_loss: 1.1920 - val_acc: 0.6417\n",
      "Epoch 11/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.2997 - acc: 0.8840 - val_loss: 1.3686 - val_acc: 0.6326\n",
      "Epoch 12/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.2647 - acc: 0.8941 - val_loss: 1.4254 - val_acc: 0.6407\n",
      "Epoch 13/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.2569 - acc: 0.8939 - val_loss: 1.5675 - val_acc: 0.6457\n",
      "Epoch 14/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.2272 - acc: 0.9048 - val_loss: 1.6413 - val_acc: 0.6265\n",
      "Epoch 15/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.2074 - acc: 0.9146 - val_loss: 1.8376 - val_acc: 0.6356\n",
      "Epoch 16/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.2017 - acc: 0.9144 - val_loss: 1.8049 - val_acc: 0.6275\n",
      "Epoch 17/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1966 - acc: 0.9141 - val_loss: 1.8083 - val_acc: 0.6255\n",
      "Epoch 18/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1827 - acc: 0.9205 - val_loss: 1.8989 - val_acc: 0.6316\n",
      "Epoch 19/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1800 - acc: 0.9230 - val_loss: 1.8625 - val_acc: 0.6316\n",
      "Epoch 20/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1743 - acc: 0.9258 - val_loss: 2.0671 - val_acc: 0.6255\n",
      "Epoch 21/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1731 - acc: 0.9210 - val_loss: 1.7699 - val_acc: 0.6417\n",
      "Epoch 22/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1579 - acc: 0.9314 - val_loss: 2.1325 - val_acc: 0.6366\n",
      "Epoch 23/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1729 - acc: 0.9250 - val_loss: 2.0570 - val_acc: 0.6397\n",
      "Epoch 24/125\n",
      "3948/3948 [==============================] - 238s - loss: 1.1057 - acc: 0.5263 - val_loss: 1.1222 - val_acc: 0.5668\n",
      "Epoch 25/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.4990 - acc: 0.8022 - val_loss: 1.3836 - val_acc: 0.6326\n",
      "Epoch 26/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.2809 - acc: 0.8888 - val_loss: 1.5527 - val_acc: 0.6377\n",
      "Epoch 27/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.2363 - acc: 0.9058 - val_loss: 1.7992 - val_acc: 0.6235\n",
      "Epoch 28/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.2022 - acc: 0.9179 - val_loss: 1.9043 - val_acc: 0.6184\n",
      "Epoch 29/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1883 - acc: 0.9195 - val_loss: 1.8239 - val_acc: 0.6397\n",
      "Epoch 30/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1684 - acc: 0.9273 - val_loss: 1.9227 - val_acc: 0.6346\n",
      "Epoch 31/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1596 - acc: 0.9296 - val_loss: 1.9556 - val_acc: 0.6417\n",
      "Epoch 32/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1533 - acc: 0.9326 - val_loss: 2.1018 - val_acc: 0.6285\n",
      "Epoch 33/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1493 - acc: 0.9324 - val_loss: 2.0673 - val_acc: 0.6123\n",
      "Epoch 34/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1413 - acc: 0.9367 - val_loss: 2.3047 - val_acc: 0.6437\n",
      "Epoch 35/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1336 - acc: 0.9377 - val_loss: 2.3337 - val_acc: 0.6296\n",
      "Epoch 36/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1324 - acc: 0.9400 - val_loss: 2.2702 - val_acc: 0.6488\n",
      "Epoch 37/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1290 - acc: 0.9405 - val_loss: 2.4754 - val_acc: 0.6397\n",
      "Epoch 38/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1297 - acc: 0.9395 - val_loss: 2.3506 - val_acc: 0.6397\n",
      "Epoch 39/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1252 - acc: 0.9402 - val_loss: 2.4566 - val_acc: 0.6366\n",
      "Epoch 40/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1230 - acc: 0.9415 - val_loss: 2.4239 - val_acc: 0.6255\n",
      "Epoch 41/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1223 - acc: 0.9428 - val_loss: 2.8128 - val_acc: 0.6093\n",
      "Epoch 42/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1231 - acc: 0.9412 - val_loss: 2.6248 - val_acc: 0.6245\n",
      "Epoch 43/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1221 - acc: 0.9410 - val_loss: 2.7108 - val_acc: 0.6407\n",
      "Epoch 44/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1220 - acc: 0.9387 - val_loss: 2.5750 - val_acc: 0.6316\n",
      "Epoch 45/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1275 - acc: 0.9417 - val_loss: 2.5399 - val_acc: 0.6397\n",
      "Epoch 46/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1241 - acc: 0.9445 - val_loss: 2.7866 - val_acc: 0.6245\n",
      "Epoch 47/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1202 - acc: 0.9405 - val_loss: 2.6805 - val_acc: 0.6316\n",
      "Epoch 48/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1141 - acc: 0.9405 - val_loss: 2.7974 - val_acc: 0.6366\n",
      "Epoch 49/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1146 - acc: 0.9453 - val_loss: 2.7332 - val_acc: 0.6245\n",
      "Epoch 50/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1122 - acc: 0.9417 - val_loss: 2.8530 - val_acc: 0.6275\n",
      "Epoch 51/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1141 - acc: 0.9443 - val_loss: 2.8544 - val_acc: 0.6417\n",
      "Epoch 52/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1101 - acc: 0.9460 - val_loss: 2.7694 - val_acc: 0.6407\n",
      "Epoch 53/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1143 - acc: 0.9440 - val_loss: 2.8704 - val_acc: 0.6366\n",
      "Epoch 54/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1140 - acc: 0.9422 - val_loss: 2.9689 - val_acc: 0.6194\n",
      "Epoch 55/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1127 - acc: 0.9450 - val_loss: 2.7959 - val_acc: 0.6275\n",
      "Epoch 56/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1256 - acc: 0.9415 - val_loss: 2.9358 - val_acc: 0.6164\n",
      "Epoch 57/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1291 - acc: 0.9425 - val_loss: 2.5290 - val_acc: 0.6417\n",
      "Epoch 58/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1238 - acc: 0.9407 - val_loss: 2.8559 - val_acc: 0.6326\n",
      "Epoch 59/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1206 - acc: 0.9405 - val_loss: 2.8265 - val_acc: 0.6123\n",
      "Epoch 60/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1128 - acc: 0.9443 - val_loss: 2.9242 - val_acc: 0.6275\n",
      "Epoch 61/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1091 - acc: 0.9460 - val_loss: 3.0993 - val_acc: 0.6204\n",
      "Epoch 62/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1065 - acc: 0.9445 - val_loss: 3.0226 - val_acc: 0.6174\n",
      "Epoch 63/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1063 - acc: 0.9463 - val_loss: 3.0818 - val_acc: 0.6356\n",
      "Epoch 64/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1043 - acc: 0.9476 - val_loss: 3.1067 - val_acc: 0.6377\n",
      "Epoch 65/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1047 - acc: 0.9476 - val_loss: 3.1538 - val_acc: 0.6265\n",
      "Epoch 66/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1035 - acc: 0.9438 - val_loss: 3.1366 - val_acc: 0.6478\n",
      "Epoch 67/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1038 - acc: 0.9440 - val_loss: 2.9713 - val_acc: 0.6488\n",
      "Epoch 68/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1031 - acc: 0.9471 - val_loss: 3.1730 - val_acc: 0.6397\n",
      "Epoch 69/125\n",
      "3948/3948 [==============================] - 238s - loss: 0.1031 - acc: 0.9468 - val_loss: 3.1123 - val_acc: 0.6366\n",
      "Epoch 70/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1043 - acc: 0.9466 - val_loss: 3.0253 - val_acc: 0.6225\n",
      "Epoch 71/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1033 - acc: 0.9460 - val_loss: 3.2089 - val_acc: 0.6387\n",
      "Epoch 72/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1028 - acc: 0.9478 - val_loss: 3.2050 - val_acc: 0.6387\n",
      "Epoch 73/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1023 - acc: 0.9471 - val_loss: 3.2617 - val_acc: 0.6508\n",
      "Epoch 74/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1020 - acc: 0.9458 - val_loss: 3.0902 - val_acc: 0.6498\n",
      "Epoch 75/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1017 - acc: 0.9471 - val_loss: 3.2387 - val_acc: 0.6478\n",
      "Epoch 76/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1017 - acc: 0.9458 - val_loss: 3.1653 - val_acc: 0.6194\n",
      "Epoch 77/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1039 - acc: 0.9453 - val_loss: 3.3088 - val_acc: 0.6255\n",
      "Epoch 78/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1065 - acc: 0.9491 - val_loss: 3.1519 - val_acc: 0.6215\n",
      "Epoch 79/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1015 - acc: 0.9483 - val_loss: 3.2914 - val_acc: 0.6356\n",
      "Epoch 80/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1035 - acc: 0.9445 - val_loss: 3.1921 - val_acc: 0.6306\n",
      "Epoch 81/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1020 - acc: 0.9468 - val_loss: 3.3091 - val_acc: 0.6285\n",
      "Epoch 82/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1074 - acc: 0.9468 - val_loss: 3.2676 - val_acc: 0.6306\n",
      "Epoch 83/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1020 - acc: 0.9453 - val_loss: 3.2728 - val_acc: 0.6377\n",
      "Epoch 84/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1014 - acc: 0.9466 - val_loss: 3.3025 - val_acc: 0.6457\n",
      "Epoch 85/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1007 - acc: 0.9488 - val_loss: 3.3185 - val_acc: 0.6417\n",
      "Epoch 86/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1014 - acc: 0.9450 - val_loss: 3.2688 - val_acc: 0.6447\n",
      "Epoch 87/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1013 - acc: 0.9473 - val_loss: 3.3133 - val_acc: 0.6407\n",
      "Epoch 88/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1009 - acc: 0.9473 - val_loss: 3.1098 - val_acc: 0.6518\n",
      "Epoch 89/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1003 - acc: 0.9443 - val_loss: 3.4243 - val_acc: 0.6296\n",
      "Epoch 90/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0997 - acc: 0.9478 - val_loss: 3.3142 - val_acc: 0.6387\n",
      "Epoch 91/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1710 - acc: 0.9324 - val_loss: 2.5879 - val_acc: 0.5931\n",
      "Epoch 92/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1909 - acc: 0.9189 - val_loss: 2.5880 - val_acc: 0.6184\n",
      "Epoch 93/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1327 - acc: 0.9392 - val_loss: 2.5904 - val_acc: 0.6204\n",
      "Epoch 94/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1223 - acc: 0.9450 - val_loss: 2.7805 - val_acc: 0.6215\n",
      "Epoch 95/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1092 - acc: 0.9460 - val_loss: 2.9868 - val_acc: 0.6113\n",
      "Epoch 96/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1089 - acc: 0.9476 - val_loss: 3.0736 - val_acc: 0.6184\n",
      "Epoch 97/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1035 - acc: 0.9468 - val_loss: 3.2309 - val_acc: 0.6144\n",
      "Epoch 98/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1026 - acc: 0.9463 - val_loss: 3.1080 - val_acc: 0.6083\n",
      "Epoch 99/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1008 - acc: 0.9455 - val_loss: 3.3867 - val_acc: 0.6144\n",
      "Epoch 100/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0999 - acc: 0.9481 - val_loss: 3.3453 - val_acc: 0.6174\n",
      "Epoch 101/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0995 - acc: 0.9476 - val_loss: 3.3329 - val_acc: 0.6154\n",
      "Epoch 102/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0995 - acc: 0.9468 - val_loss: 3.4062 - val_acc: 0.6113\n",
      "Epoch 103/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.1000 - acc: 0.9498 - val_loss: 3.4383 - val_acc: 0.6154\n",
      "Epoch 104/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0990 - acc: 0.9468 - val_loss: 3.3799 - val_acc: 0.6164\n",
      "Epoch 105/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0985 - acc: 0.9478 - val_loss: 3.4536 - val_acc: 0.6154\n",
      "Epoch 106/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0988 - acc: 0.9473 - val_loss: 3.4679 - val_acc: 0.6225\n",
      "Epoch 107/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0986 - acc: 0.9493 - val_loss: 3.4290 - val_acc: 0.6174\n",
      "Epoch 108/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0989 - acc: 0.9463 - val_loss: 3.4467 - val_acc: 0.6194\n",
      "Epoch 109/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0985 - acc: 0.9468 - val_loss: 3.4514 - val_acc: 0.6245\n",
      "Epoch 110/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0998 - acc: 0.9491 - val_loss: 3.4867 - val_acc: 0.6154\n",
      "Epoch 111/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0990 - acc: 0.9473 - val_loss: 3.4816 - val_acc: 0.6235\n",
      "Epoch 112/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0986 - acc: 0.9466 - val_loss: 3.5838 - val_acc: 0.6144\n",
      "Epoch 113/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0993 - acc: 0.9486 - val_loss: 3.2709 - val_acc: 0.6235\n",
      "Epoch 114/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0990 - acc: 0.9468 - val_loss: 3.5413 - val_acc: 0.6194\n",
      "Epoch 115/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0993 - acc: 0.9466 - val_loss: 3.5169 - val_acc: 0.6235\n",
      "Epoch 116/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0983 - acc: 0.9491 - val_loss: 3.4285 - val_acc: 0.6235\n",
      "Epoch 117/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0988 - acc: 0.9493 - val_loss: 3.5268 - val_acc: 0.6225\n",
      "Epoch 118/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0980 - acc: 0.9473 - val_loss: 3.5197 - val_acc: 0.6204\n",
      "Epoch 119/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0986 - acc: 0.9506 - val_loss: 3.5414 - val_acc: 0.6275\n",
      "Epoch 120/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0979 - acc: 0.9506 - val_loss: 3.5388 - val_acc: 0.6235\n",
      "Epoch 121/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0980 - acc: 0.9483 - val_loss: 3.5883 - val_acc: 0.6255\n",
      "Epoch 122/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0983 - acc: 0.9466 - val_loss: 3.6014 - val_acc: 0.6194\n",
      "Epoch 123/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0975 - acc: 0.9473 - val_loss: 3.6087 - val_acc: 0.6144\n",
      "Epoch 124/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0978 - acc: 0.9504 - val_loss: 3.5914 - val_acc: 0.6204\n",
      "Epoch 125/125\n",
      "3948/3948 [==============================] - 237s - loss: 0.0978 - acc: 0.9486 - val_loss: 3.5915 - val_acc: 0.6285\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_text, Y, \n",
    "                 batch_size=16, epochs=125, verbose=1, \n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.651821860176349\n"
     ]
    }
   ],
   "source": [
    "acc2=max(hist.history['val_acc'])\n",
    "print(acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2736"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 500, 300)          821100    \n",
      "_________________________________________________________________\n",
      "lstm_62 (LSTM)               (None, 500, 512)          1665024   \n",
      "_________________________________________________________________\n",
      "lstm_63 (LSTM)               (None, 500, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "lstm_64 (LSTM)               (None, 500, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "lstm_65 (LSTM)               (None, 500, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "lstm_66 (LSTM)               (None, 500, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "lstm_67 (LSTM)               (None, 500, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "lstm_68 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 4)                 4100      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 15,610,736\n",
      "Trainable params: 15,610,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model3 Built\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(nb_words,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights = [g_word_embedding_matrix],\n",
    "                    input_length = MAX_SEQUENCE_LENGTH,\n",
    "                    trainable = True))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model3 Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3948 samples, validate on 988 samples\n",
      "Epoch 1/100\n",
      "3948/3948 [==============================] - 518s - loss: 1.3810 - acc: 0.3402 - val_loss: 1.3699 - val_acc: 0.3573\n",
      "Epoch 2/100\n",
      "3948/3948 [==============================] - 514s - loss: 1.3733 - acc: 0.3468 - val_loss: 1.3463 - val_acc: 0.3968\n",
      "Epoch 3/100\n",
      "3948/3948 [==============================] - 510s - loss: 1.3744 - acc: 0.3506 - val_loss: 1.3641 - val_acc: 0.3694\n",
      "Epoch 4/100\n",
      "3948/3948 [==============================] - 510s - loss: 1.3681 - acc: 0.3503 - val_loss: 1.3530 - val_acc: 0.3846\n",
      "Epoch 5/100\n",
      "3948/3948 [==============================] - 510s - loss: 1.3298 - acc: 0.3944 - val_loss: 1.3535 - val_acc: 0.3785\n",
      "Epoch 6/100\n",
      "3948/3948 [==============================] - 510s - loss: 1.2460 - acc: 0.4276 - val_loss: 1.2699 - val_acc: 0.4443\n",
      "Epoch 7/100\n",
      "3948/3948 [==============================] - 510s - loss: 1.1412 - acc: 0.4934 - val_loss: 1.2595 - val_acc: 0.4575\n",
      "Epoch 8/100\n",
      "3948/3948 [==============================] - 510s - loss: 1.0437 - acc: 0.5448 - val_loss: 1.2353 - val_acc: 0.4555\n",
      "Epoch 9/100\n",
      "3948/3948 [==============================] - 511s - loss: 0.9380 - acc: 0.5849 - val_loss: 1.1951 - val_acc: 0.5030\n",
      "Epoch 10/100\n",
      "3948/3948 [==============================] - 510s - loss: 0.8436 - acc: 0.6348 - val_loss: 1.3192 - val_acc: 0.5202\n",
      "Epoch 11/100\n",
      "3948/3948 [==============================] - 510s - loss: 0.7884 - acc: 0.6543 - val_loss: 1.3001 - val_acc: 0.4838\n",
      "Epoch 12/100\n",
      "3948/3948 [==============================] - 510s - loss: 0.6989 - acc: 0.7044 - val_loss: 1.3822 - val_acc: 0.5749\n",
      "Epoch 13/100\n",
      "3948/3948 [==============================] - 510s - loss: 0.6315 - acc: 0.7477 - val_loss: 1.3008 - val_acc: 0.6113\n",
      "Epoch 14/100\n",
      "3948/3948 [==============================] - 511s - loss: 0.5602 - acc: 0.7824 - val_loss: 1.4261 - val_acc: 0.5719\n",
      "Epoch 15/100\n",
      "3948/3948 [==============================] - 511s - loss: 0.4906 - acc: 0.8153 - val_loss: 1.4607 - val_acc: 0.6032\n",
      "Epoch 16/100\n",
      "3948/3948 [==============================] - 511s - loss: 0.4296 - acc: 0.8397 - val_loss: 1.4594 - val_acc: 0.6164\n",
      "Epoch 17/100\n",
      "3948/3948 [==============================] - 511s - loss: 0.3824 - acc: 0.8549 - val_loss: 1.5028 - val_acc: 0.6326\n",
      "Epoch 18/100\n",
      "3948/3948 [==============================] - 511s - loss: 0.3484 - acc: 0.8706 - val_loss: 1.5231 - val_acc: 0.6083\n",
      "Epoch 19/100\n",
      "3948/3948 [==============================] - 510s - loss: 0.3015 - acc: 0.8870 - val_loss: 1.6795 - val_acc: 0.6417\n",
      "Epoch 20/100\n",
      "3948/3948 [==============================] - 510s - loss: 0.2731 - acc: 0.8974 - val_loss: 1.7599 - val_acc: 0.6134\n",
      "Epoch 21/100\n",
      "3948/3948 [==============================] - 510s - loss: 0.2624 - acc: 0.8961 - val_loss: 1.6985 - val_acc: 0.6215\n",
      "Epoch 22/100\n",
      "3948/3948 [==============================] - 510s - loss: 0.2519 - acc: 0.8972 - val_loss: 1.8391 - val_acc: 0.6296\n",
      "Epoch 23/100\n",
      "3948/3948 [==============================] - 510s - loss: 0.2482 - acc: 0.9007 - val_loss: 1.8337 - val_acc: 0.6215\n",
      "Epoch 24/100\n",
      "3948/3948 [==============================] - 510s - loss: 0.2593 - acc: 0.8997 - val_loss: 1.7684 - val_acc: 0.6194\n",
      "Epoch 25/100\n",
      "3948/3948 [==============================] - 510s - loss: 0.2255 - acc: 0.9083 - val_loss: 1.8009 - val_acc: 0.6225\n",
      "Epoch 26/100\n",
      "3948/3948 [==============================] - 511s - loss: 0.2119 - acc: 0.9136 - val_loss: 1.8708 - val_acc: 0.6285\n",
      "Epoch 27/100\n",
      "3948/3948 [==============================] - 509s - loss: 0.1865 - acc: 0.9212 - val_loss: 2.0440 - val_acc: 0.6245\n",
      "Epoch 28/100\n",
      "3948/3948 [==============================] - 509s - loss: 0.1716 - acc: 0.9281 - val_loss: 2.1043 - val_acc: 0.6174\n",
      "Epoch 29/100\n",
      "3948/3948 [==============================] - 508s - loss: 0.1596 - acc: 0.9283 - val_loss: 2.1066 - val_acc: 0.6073\n",
      "Epoch 30/100\n",
      "3948/3948 [==============================] - 508s - loss: 0.1564 - acc: 0.9336 - val_loss: 2.1638 - val_acc: 0.6103\n",
      "Epoch 31/100\n",
      "3948/3948 [==============================] - 517s - loss: 0.1476 - acc: 0.9367 - val_loss: 2.3781 - val_acc: 0.6154\n",
      "Epoch 32/100\n",
      "3948/3948 [==============================] - 518s - loss: 0.1397 - acc: 0.9415 - val_loss: 2.4684 - val_acc: 0.6012\n",
      "Epoch 33/100\n",
      "3948/3948 [==============================] - 519s - loss: 0.1449 - acc: 0.9352 - val_loss: 2.3317 - val_acc: 0.6134\n",
      "Epoch 34/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.2065 - acc: 0.9291 - val_loss: 2.2773 - val_acc: 0.4949\n",
      "Epoch 35/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.2693 - acc: 0.8964 - val_loss: 2.0627 - val_acc: 0.6336\n",
      "Epoch 36/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.2027 - acc: 0.9207 - val_loss: 1.8542 - val_acc: 0.5941\n",
      "Epoch 37/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1740 - acc: 0.9291 - val_loss: 2.0156 - val_acc: 0.6255\n",
      "Epoch 38/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1684 - acc: 0.9362 - val_loss: 1.9592 - val_acc: 0.6235\n",
      "Epoch 39/100\n",
      "3948/3948 [==============================] - 519s - loss: 0.1521 - acc: 0.9344 - val_loss: 2.1737 - val_acc: 0.6134\n",
      "Epoch 40/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1550 - acc: 0.9349 - val_loss: 2.1205 - val_acc: 0.6144\n",
      "Epoch 41/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1348 - acc: 0.9367 - val_loss: 2.7568 - val_acc: 0.5698\n",
      "Epoch 42/100\n",
      "3948/3948 [==============================] - 521s - loss: 0.1586 - acc: 0.9336 - val_loss: 2.1914 - val_acc: 0.6174\n",
      "Epoch 43/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1520 - acc: 0.9359 - val_loss: 2.1057 - val_acc: 0.6225\n",
      "Epoch 44/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1361 - acc: 0.9384 - val_loss: 2.1951 - val_acc: 0.6053\n",
      "Epoch 45/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1426 - acc: 0.9359 - val_loss: 2.1233 - val_acc: 0.6245\n",
      "Epoch 46/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1312 - acc: 0.9395 - val_loss: 2.3512 - val_acc: 0.6245\n",
      "Epoch 47/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1251 - acc: 0.9402 - val_loss: 2.5095 - val_acc: 0.6002\n",
      "Epoch 48/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1227 - acc: 0.9392 - val_loss: 2.6099 - val_acc: 0.5891\n",
      "Epoch 49/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1179 - acc: 0.9440 - val_loss: 2.7846 - val_acc: 0.6113\n",
      "Epoch 50/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1163 - acc: 0.9420 - val_loss: 2.9129 - val_acc: 0.6053\n",
      "Epoch 51/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1145 - acc: 0.9395 - val_loss: 2.7950 - val_acc: 0.6144\n",
      "Epoch 52/100\n",
      "3948/3948 [==============================] - 519s - loss: 0.1140 - acc: 0.9466 - val_loss: 3.0160 - val_acc: 0.6053\n",
      "Epoch 53/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1126 - acc: 0.9425 - val_loss: 3.0639 - val_acc: 0.6134\n",
      "Epoch 54/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1097 - acc: 0.9445 - val_loss: 3.2305 - val_acc: 0.6113\n",
      "Epoch 55/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1143 - acc: 0.9415 - val_loss: 3.0519 - val_acc: 0.6164\n",
      "Epoch 56/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1110 - acc: 0.9417 - val_loss: 2.9110 - val_acc: 0.6053\n",
      "Epoch 57/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1118 - acc: 0.9468 - val_loss: 3.0650 - val_acc: 0.6063\n",
      "Epoch 58/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1099 - acc: 0.9420 - val_loss: 3.3129 - val_acc: 0.5870\n",
      "Epoch 59/100\n",
      "3948/3948 [==============================] - 518s - loss: 0.1115 - acc: 0.9443 - val_loss: 3.1245 - val_acc: 0.6083\n",
      "Epoch 60/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1085 - acc: 0.9450 - val_loss: 3.1958 - val_acc: 0.6103\n",
      "Epoch 61/100\n",
      "3948/3948 [==============================] - 521s - loss: 0.1085 - acc: 0.9471 - val_loss: 3.1234 - val_acc: 0.6265\n",
      "Epoch 62/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1074 - acc: 0.9450 - val_loss: 3.4418 - val_acc: 0.6083\n",
      "Epoch 63/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1076 - acc: 0.9448 - val_loss: 3.1372 - val_acc: 0.6002\n",
      "Epoch 64/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1105 - acc: 0.9428 - val_loss: 3.3749 - val_acc: 0.5951\n",
      "Epoch 65/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1084 - acc: 0.9448 - val_loss: 3.3250 - val_acc: 0.6144\n",
      "Epoch 66/100\n",
      "3948/3948 [==============================] - 520s - loss: 0.1117 - acc: 0.9458 - val_loss: 3.2396 - val_acc: 0.6103\n",
      "Epoch 67/100\n",
      "3948/3948 [==============================] - 521s - loss: 0.1186 - acc: 0.9435 - val_loss: 3.3057 - val_acc: 0.6093\n",
      "Epoch 68/100\n",
      "3948/3948 [==============================] - 521s - loss: 1.3030 - acc: 0.5415 - val_loss: 1.3548 - val_acc: 0.3694\n",
      "Epoch 69/100\n",
      "3948/3948 [==============================] - 521s - loss: 1.3663 - acc: 0.3402 - val_loss: 1.3707 - val_acc: 0.3694\n",
      "Epoch 70/100\n",
      "3948/3948 [==============================] - 520s - loss: 1.3659 - acc: 0.3402 - val_loss: 1.3610 - val_acc: 0.3694\n",
      "Epoch 71/100\n",
      "3948/3948 [==============================] - 520s - loss: 1.3657 - acc: 0.3402 - val_loss: 1.3653 - val_acc: 0.3694\n",
      "Epoch 72/100\n",
      "3948/3948 [==============================] - 520s - loss: 1.3658 - acc: 0.3402 - val_loss: 1.3690 - val_acc: 0.3694\n",
      "Epoch 73/100\n",
      "3948/3948 [==============================] - 520s - loss: 1.3657 - acc: 0.3402 - val_loss: 1.3655 - val_acc: 0.3694\n",
      "Epoch 74/100\n",
      "3948/3948 [==============================] - 519s - loss: 1.3649 - acc: 0.3402 - val_loss: 1.3686 - val_acc: 0.3694\n",
      "Epoch 75/100\n",
      "3948/3948 [==============================] - 519s - loss: 1.3654 - acc: 0.3402 - val_loss: 1.3659 - val_acc: 0.3694\n",
      "Epoch 76/100\n",
      "3948/3948 [==============================] - 520s - loss: 1.3648 - acc: 0.3402 - val_loss: 1.3649 - val_acc: 0.3694\n",
      "Epoch 77/100\n",
      "3948/3948 [==============================] - 519s - loss: 1.3651 - acc: 0.3402 - val_loss: 1.3634 - val_acc: 0.3694\n",
      "Epoch 78/100\n",
      "3948/3948 [==============================] - 520s - loss: 1.3647 - acc: 0.3402 - val_loss: 1.3642 - val_acc: 0.3694\n",
      "Epoch 79/100\n",
      "3948/3948 [==============================] - 519s - loss: 1.3651 - acc: 0.3402 - val_loss: 1.3625 - val_acc: 0.3694\n",
      "Epoch 80/100\n",
      "3948/3948 [==============================] - 519s - loss: 1.3649 - acc: 0.3402 - val_loss: 1.3646 - val_acc: 0.3694\n",
      "Epoch 81/100\n",
      "3948/3948 [==============================] - 520s - loss: 1.3651 - acc: 0.3402 - val_loss: 1.3646 - val_acc: 0.3694\n",
      "Epoch 82/100\n",
      "3948/3948 [==============================] - 519s - loss: 1.3649 - acc: 0.3402 - val_loss: 1.3624 - val_acc: 0.3694\n",
      "Epoch 83/100\n",
      "3948/3948 [==============================] - 519s - loss: 1.3648 - acc: 0.3402 - val_loss: 1.3624 - val_acc: 0.3694\n",
      "Epoch 84/100\n",
      "3948/3948 [==============================] - 519s - loss: 1.3648 - acc: 0.3402 - val_loss: 1.3642 - val_acc: 0.3694\n",
      "Epoch 85/100\n",
      "3948/3948 [==============================] - 520s - loss: 1.3647 - acc: 0.3402 - val_loss: 1.3651 - val_acc: 0.3694\n",
      "Epoch 86/100\n",
      "3948/3948 [==============================] - 519s - loss: 1.3649 - acc: 0.3402 - val_loss: 1.3638 - val_acc: 0.3694\n",
      "Epoch 87/100\n",
      "3948/3948 [==============================] - 520s - loss: 1.3649 - acc: 0.3402 - val_loss: 1.3641 - val_acc: 0.3694\n",
      "Epoch 88/100\n",
      "3948/3948 [==============================] - 519s - loss: 1.3648 - acc: 0.3402 - val_loss: 1.3633 - val_acc: 0.3694\n",
      "Epoch 89/100\n",
      "3948/3948 [==============================] - 519s - loss: 1.3648 - acc: 0.3402 - val_loss: 1.3632 - val_acc: 0.3694\n",
      "Epoch 90/100\n",
      "3948/3948 [==============================] - 519s - loss: 1.3646 - acc: 0.3402 - val_loss: 1.3656 - val_acc: 0.3694\n",
      "Epoch 91/100\n",
      "3948/3948 [==============================] - 519s - loss: 1.3649 - acc: 0.3402 - val_loss: 1.3652 - val_acc: 0.3694\n",
      "Epoch 92/100\n",
      "3948/3948 [==============================] - 520s - loss: 1.3649 - acc: 0.3402 - val_loss: 1.3641 - val_acc: 0.3694\n",
      "Epoch 93/100\n",
      "3948/3948 [==============================] - 520s - loss: 1.3647 - acc: 0.3402 - val_loss: 1.3647 - val_acc: 0.3694\n",
      "Epoch 94/100\n",
      "3948/3948 [==============================] - 520s - loss: 1.3647 - acc: 0.3402 - val_loss: 1.3646 - val_acc: 0.3694\n",
      "Epoch 95/100\n",
      "3948/3948 [==============================] - 519s - loss: 1.3647 - acc: 0.3402 - val_loss: 1.3638 - val_acc: 0.3694\n",
      "Epoch 96/100\n",
      "3948/3948 [==============================] - 519s - loss: 1.3646 - acc: 0.3402 - val_loss: 1.3635 - val_acc: 0.3694\n",
      "Epoch 97/100\n",
      "3948/3948 [==============================] - 518s - loss: 1.3645 - acc: 0.3402 - val_loss: 1.3650 - val_acc: 0.3694\n",
      "Epoch 98/100\n",
      "3948/3948 [==============================] - 518s - loss: 1.3646 - acc: 0.3402 - val_loss: 1.3645 - val_acc: 0.3694\n",
      "Epoch 99/100\n",
      "3948/3948 [==============================] - 520s - loss: 1.3647 - acc: 0.3402 - val_loss: 1.3638 - val_acc: 0.3694\n",
      "Epoch 100/100\n",
      "3948/3948 [==============================] - 519s - loss: 1.3646 - acc: 0.3402 - val_loss: 1.3645 - val_acc: 0.3694\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_text, Y, \n",
    "                 batch_size=64, epochs=125, verbose=1, \n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6417004002733269\n"
     ]
    }
   ],
   "source": [
    "acc3=max(hist.history['val_acc'])\n",
    "print(acc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 300)          821100    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 500, 128)          219648    \n",
      "_________________________________________________________________\n",
      "AttentionDecoder (AttentionD (None, 500, 128)          246528    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               32768512  \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 34,057,840\n",
      "Trainable params: 34,057,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Try BLSTM with attention model\n",
    "\n",
    "def attention_model(optimizer='rmsprop'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(nb_words,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights = [g_word_embedding_matrix],\n",
    "                    input_length = MAX_SEQUENCE_LENGTH,\n",
    "                    trainable = True))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(AttentionDecoder(128, 128))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model4 = attention_model()\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3948 samples, validate on 988 samples\n",
      "Epoch 1/25\n",
      "3948/3948 [==============================] - 854s - loss: 1.5641 - acc: 0.3070 - val_loss: 1.4088 - val_acc: 0.3694\n",
      "Epoch 2/25\n",
      "3948/3948 [==============================] - 847s - loss: 1.4590 - acc: 0.2966 - val_loss: 1.5173 - val_acc: 0.1630\n",
      "Epoch 3/25\n",
      "3948/3948 [==============================] - 843s - loss: 1.4092 - acc: 0.3625 - val_loss: 1.3225 - val_acc: 0.3472\n",
      "Epoch 4/25\n",
      "3948/3948 [==============================] - 843s - loss: 1.1294 - acc: 0.5233 - val_loss: 1.0990 - val_acc: 0.5729\n",
      "Epoch 5/25\n",
      "3948/3948 [==============================] - 854s - loss: 0.9794 - acc: 0.6102 - val_loss: 0.9835 - val_acc: 0.5992\n",
      "Epoch 6/25\n",
      "3948/3948 [==============================] - 805s - loss: 0.8652 - acc: 0.6535 - val_loss: 0.9351 - val_acc: 0.6326\n",
      "Epoch 7/25\n",
      "3948/3948 [==============================] - 803s - loss: 0.7666 - acc: 0.7097 - val_loss: 1.0101 - val_acc: 0.6437\n",
      "Epoch 8/25\n",
      "3948/3948 [==============================] - 1076s - loss: 0.6973 - acc: 0.7343 - val_loss: 0.9715 - val_acc: 0.6518\n",
      "Epoch 9/25\n",
      "3948/3948 [==============================] - 1075s - loss: 0.6288 - acc: 0.7685 - val_loss: 1.0412 - val_acc: 0.6609\n",
      "Epoch 10/25\n",
      "3948/3948 [==============================] - 1075s - loss: 0.5529 - acc: 0.7867 - val_loss: 1.0759 - val_acc: 0.6265\n",
      "Epoch 11/25\n",
      "3948/3948 [==============================] - 1076s - loss: 0.5184 - acc: 0.8055 - val_loss: 1.1222 - val_acc: 0.6478\n",
      "Epoch 12/25\n",
      "3948/3948 [==============================] - 1074s - loss: 0.4813 - acc: 0.8184 - val_loss: 1.1775 - val_acc: 0.6204\n",
      "Epoch 13/25\n",
      "3948/3948 [==============================] - 1075s - loss: 0.4396 - acc: 0.8333 - val_loss: 1.4527 - val_acc: 0.6326\n",
      "Epoch 14/25\n",
      "3948/3948 [==============================] - 967s - loss: 0.3985 - acc: 0.8425 - val_loss: 1.3894 - val_acc: 0.5962\n",
      "Epoch 15/25\n",
      "3948/3948 [==============================] - 968s - loss: 0.3657 - acc: 0.8546 - val_loss: 1.4805 - val_acc: 0.6123\n",
      "Epoch 16/25\n",
      "3948/3948 [==============================] - 968s - loss: 0.3509 - acc: 0.8637 - val_loss: 1.6474 - val_acc: 0.6387\n",
      "Epoch 17/25\n",
      "3948/3948 [==============================] - 913s - loss: 0.3317 - acc: 0.8734 - val_loss: 1.6899 - val_acc: 0.6296\n",
      "Epoch 18/25\n",
      "3948/3948 [==============================] - 1009s - loss: 0.3040 - acc: 0.8782 - val_loss: 1.7373 - val_acc: 0.6204\n",
      "Epoch 19/25\n",
      "3948/3948 [==============================] - 1006s - loss: 0.2913 - acc: 0.8751 - val_loss: 1.7453 - val_acc: 0.6377\n",
      "Epoch 20/25\n",
      "3948/3948 [==============================] - 846s - loss: 0.2778 - acc: 0.8898 - val_loss: 1.6110 - val_acc: 0.6346\n",
      "Epoch 21/25\n",
      "3948/3948 [==============================] - 844s - loss: 0.2733 - acc: 0.8888 - val_loss: 2.1083 - val_acc: 0.6285\n",
      "Epoch 22/25\n",
      "3948/3948 [==============================] - 847s - loss: 0.2655 - acc: 0.8992 - val_loss: 2.0025 - val_acc: 0.6012\n",
      "Epoch 23/25\n",
      "3948/3948 [==============================] - 848s - loss: 0.2435 - acc: 0.8946 - val_loss: 2.1224 - val_acc: 0.6265\n",
      "Epoch 24/25\n",
      "3948/3948 [==============================] - 846s - loss: 0.2499 - acc: 0.8969 - val_loss: 1.9218 - val_acc: 0.6255\n",
      "Epoch 25/25\n",
      "3948/3948 [==============================] - 937s - loss: 0.2230 - acc: 0.9078 - val_loss: 2.7716 - val_acc: 0.5840\n"
     ]
    }
   ],
   "source": [
    "hist4 = model4.fit(x_train_text, Y, batch_size=64, epochs=125, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6609311743303832\n"
     ]
    }
   ],
   "source": [
    "acc4=max(hist4.history['val_acc'])\n",
    "print(acc4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "er",
   "language": "python",
   "name": "er"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
